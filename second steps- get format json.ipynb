{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b415951c-8d11-4969-aa03-09693cbb8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# אז מה צריך לעשות ?\n",
    "# כל תמונה דבר ראשון רצים עליה ומסווגים לאיזה סוג היא:\n",
    "# תמונה- אז רק מכניסים את הנתיב שלה  לDB לטבלת תמונות\n",
    "# אם זה כירכת יד ושם- אז שולחים לפונקציה ש "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5b86b6-6e89-46ee-9d0f-17ddb4fa4b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1df835d-48d6-4c18-a056-e8abde667547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf3184c-e889-4f79-80e4-103c95cc6c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Python310\\\\python.exe -m pip install --upgrade pip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'C:\\Python310\\python.exe -m pip install --upgrade pip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a67b48f-a483-44c6-9603-ea57abfb13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e42bcb6-2e97-42ec-b1a9-2841e91b75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-image\n",
    "# # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98cff1f-d05b-444c-8fcd-951c34060d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345cc99f-c933-449c-a2bc-202f813522ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade --force-reinstall tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dbeb387-cf7c-42ef-b4e3-680d700728ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7bcdc9-6747-4eee-962f-a84d5338991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecbb195c-cefa-4370-a1f7-908f2dd91415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import os, os.path\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "import glob\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import ast\n",
    "import numpy as np\n",
    "import joblib\n",
    "import ast\n",
    "from skimage.transform import resize\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import pyodbc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb5df2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4725b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_trf\n",
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6528f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3f9f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e300692b-1f6b-45e2-96dd-9e208cc9d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_text(i):\n",
    "    if i:\n",
    "    # Check and replace '.' if it exists in the string\n",
    "        if '.' in i:\n",
    "            i = i.replace('.', '')\n",
    "    \n",
    "        # Check and replace '/' if it exists in the string\n",
    "        if '/' in i:\n",
    "            i = i.replace('/', '')\n",
    "    \n",
    "        # Check and replace ',' if it exists in the string\n",
    "        if ',' in i:\n",
    "            i = i.replace(',', '')\n",
    "    \n",
    "        # Check and replace ':' if it exists in the string\n",
    "        if ':' in i:\n",
    "            i = i.replace(':', '')\n",
    "    \n",
    "        # Check and replace ')' if it exists in the string\n",
    "        if ')' in i:\n",
    "            i = i.replace(')', '')\n",
    "\n",
    "    # Check and replace '(' if it exists in the string\n",
    "        if '(' in i:\n",
    "            i = i.replace('(', '')\n",
    "\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebe72cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finds_dates_places_persons(desc):\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "    doc = nlp(desc)\n",
    "    entity_types = {}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        type_name = ent.label_\n",
    "        entity = ent.text\n",
    "\n",
    "        if type_name not in entity_types:\n",
    "            entity_types[type_name] = []\n",
    "\n",
    "        entity_types[type_name].append(entity)\n",
    "    target_word=nlp(\"study\")\n",
    "    return entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699835b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words_in_json(json):\n",
    "    textAnnotations = json['responses'][0]['textAnnotations']\n",
    "    word_count=0\n",
    "    for word in textAnnotations:\n",
    "        desc = word['description']\n",
    "        word_count+=1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1ab72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_with_vertices(data_response):\n",
    "    textAnnotations=data_response['responses'][0][\"textAnnotations\"]\n",
    "    text=data_response['responses'][0]['fullTextAnnotation']['text']\n",
    "    arr_res=[]\n",
    "    arr_res.append(({'text':text},textAnnotations[0]['boundingPoly'] ))\n",
    "    for idx, i in enumerate(textAnnotations, start=1):\n",
    "        ver=i[\"boundingPoly\"]\n",
    "        desc=i['description']\n",
    "        desc=desc.split(\"\\n\")\n",
    "        list1=(ver,desc)\n",
    "        arr_res.append(list1)\n",
    "    xx=arr_res[0]\n",
    "    arr_res= arr_res[1:]\n",
    "    return [arr_res, xx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a576acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_get_by_y(json):\n",
    "    [words,full_desc_with_verices] =get_all_words_with_vertices(json)\n",
    "    json_res={}\n",
    "    json_res[\"full_desc_with_verices\"]=full_desc_with_verices\n",
    "    json_res['words']=words\n",
    "    return json_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a7636d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_y(json):\n",
    "    vertices = json[\"full_desc_with_verices\"][1][\"vertices\"]\n",
    "    y_start = vertices[0].get(\"y\", 0)  # default value 0 if 'y' key is not found in the dictionary\n",
    "    y_end = vertices[3].get(\"y\", 0)\n",
    "    \n",
    "    x_start = vertices[0].get(\"x\", 0)  # default value 0 if 'y' key is not found in the dictionary\n",
    "    x_end = vertices[1].get(\"x\", 0)\n",
    "    if \"y\" in vertices[0] and \"y\" in vertices[3]:\n",
    "        range_y = vertices[3][\"y\"] - vertices[0][\"y\"]\n",
    "    else:\n",
    "        range_y = 0  # or set a default value if 'y' key is missing in some vertices\n",
    "    if \"x\" in vertices[1] and \"x\" in vertices[0]:\n",
    "        range_x= vertices[1][\"x\"] - vertices[0][\"x\"]\n",
    "    else:\n",
    "        range_x = 0  # or set a default value if 'y' key is missing in some vertices\n",
    "    \n",
    "    \n",
    "    if range_y != 0 and range_x!=0:\n",
    "        loc_title_up= (y_start,y_start+range_y/4)\n",
    "        loc_title_middle=(y_start+range_y/4,y_start+(range_y/3)*2)\n",
    "        loc_title_down=(y_start+(range_y/3)*2,y_start+range_y)\n",
    "        x_middle=(x_start+(range_x/2))\n",
    "        return [loc_title_up,loc_title_middle,loc_title_down, x_middle]\n",
    "    else:\n",
    "        return [(0, 0), (0, 0), (0,0), (0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72c3d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_block(block):\n",
    "    text=''\n",
    "    words=block['paragraphs'][0]['words']\n",
    "    for w in words:\n",
    "        word=w['symbols']\n",
    "        for l in word:\n",
    "            text+=l['text']\n",
    "        text+=' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "730f057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_big_letter_from_desc(json, words_count):\n",
    "    text_annotations = json.get('responses', [{}])[0].get('textAnnotations', [])\n",
    "    \n",
    "    block_areas_num = []\n",
    "    \n",
    "    for word in text_annotations[1:]:\n",
    "        desc = word.get('description', '')\n",
    "        block_vertices = word.get('boundingPoly', {}).get('vertices', [])\n",
    "        x_values = [vertex.get('x', 0) for vertex in block_vertices if 'x' in vertex]\n",
    "        y_values = [vertex.get('y', 0) for vertex in block_vertices if 'y' in vertex]\n",
    "        \n",
    "        if len(x_values) > 0 and len(y_values) > 0:\n",
    "            max_x = max(x_values)\n",
    "            min_x = min(x_values)\n",
    "\n",
    "            max_y = max(y_values)\n",
    "            min_y = min(y_values)\n",
    "\n",
    "            block_x = max_x - min_x\n",
    "            block_y = max_y - min_y\n",
    "\n",
    "            num_letter = len(desc)\n",
    "            area = block_x * block_y\n",
    "            letter = area / num_letter\n",
    "            \n",
    "            if desc not in ['(', ')', '-', '[', ']', ',', '.']:\n",
    "                block_areas_num.append((desc, letter))\n",
    "    \n",
    "    sorted_block_areas_num = sorted(block_areas_num, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_block_areas_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1fa8b04-3368-4c88-b6a7-17ac7f815cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_Testimony(json_res, json, date_arr, format_json):\n",
    "    res_date=''\n",
    "    side_date=1\n",
    "    data_Language=format_json['data_Languages'][0][0]\n",
    "    if data_Language=='Hebrew' or data_Language=='Arabic':\n",
    "        side_date=0\n",
    "    [loc_title_up,loc_title_middle,loc_title_down, x_middle ]=get_size_y(json_res)\n",
    "    print(loc_title_up)\n",
    "    if [loc_title_up,loc_title_middle,loc_title_down, x_middle ]!=[(0, 0), (0, 0), (0,0), (0, 0)]:\n",
    "        fullTextAnnotation_pages = json['responses'][0]['fullTextAnnotation']['pages']\n",
    "        dict_block_by_loc_date={0:[[], []], 1:[[], []], 2:[[], []]}\n",
    "        for page in fullTextAnnotation_pages:\n",
    "            blocks = page['blocks']\n",
    "            blocks=sorted( blocks, key=lambda block:block['boundingBox']['vertices'][0]['y'] )\n",
    "            for block in blocks:\n",
    "                # 0 left\n",
    "                # 1 right\n",
    "                side=0\n",
    "                text_from_block=get_text_from_block(block)\n",
    "                words=block['paragraphs'][0]['words']\n",
    "                block_vertices = block['boundingBox']['vertices']\n",
    "                y=block_vertices[3]['y']-block_vertices[0]['y']\n",
    "                x=block_vertices[0]['x']\n",
    "                if x>=x_middle:\n",
    "                    side=1\n",
    "                if block_vertices[0]['y']<=loc_title_up[1]:\n",
    "                    ans=0\n",
    "                elif block_vertices[0]['y']<=loc_title_middle[1]:\n",
    "                    ans=1\n",
    "                else:\n",
    "                    ans=2\n",
    "                desc_words=''\n",
    "                for word in words:\n",
    "                    word_vertices=word['boundingBox']['vertices'] \n",
    "                    desc=''\n",
    "                    word1=word['symbols']\n",
    "                    for l in word1:\n",
    "                        desc+=l['text']\n",
    "                    desc_words+=desc\n",
    "                    desc_words+=' '\n",
    "                    for date in date_arr:\n",
    "                        date_split=date.split(' ')\n",
    "                        for ds in date_split:\n",
    "                            if desc==ds:\n",
    "                                dict_block_by_loc_date[ans][side].append(date)\n",
    "    if dict_block_by_loc_date[2]:\n",
    "        arr=dict_block_by_loc_date[2]\n",
    "        res_arr=arr[side_date]\n",
    "        if len(res_arr)>0:\n",
    "            return res_arr[0]\n",
    "        else:\n",
    "            res_arr=arr[abs(side_date-1)]\n",
    "            if len(res_arr)>0:\n",
    "                return res_arr[0]\n",
    "    # elif dict_block_by_loc_date[2]:\n",
    "    #     arr=dict_block_by_loc_date[2]\n",
    "    #     res_arr=arr[side_date]\n",
    "    #     if len(res_arr)>0:\n",
    "    #         return res_arr[0]\n",
    "    #     else:\n",
    "    #         res_arr=arr[abs(side_date-1)]\n",
    "    #         if len(res_arr)>0:\n",
    "    #             return res_arr[0]\n",
    "    # elif dict_block_by_loc_date[1]:\n",
    "    #     arr=dict_block_by_loc_date[1]\n",
    "    #     res_arr=arr[side_date]\n",
    "    #     if len(res_arr)>0:\n",
    "    #         return res_arr[0]\n",
    "    #     else:\n",
    "    #         res_arr=arr[abs(side_date-1)]\n",
    "    #         if len(res_arr)>0:\n",
    "    #             return res_arr[0]\n",
    "    return '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e26dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(json_res, json, date_arr, format_json):\n",
    "    res_date=''\n",
    "    side_date=1\n",
    "    data_Language=format_json['data_Languages'][0][0]\n",
    "    if data_Language=='Hebrew' or data_Language=='Arabic':\n",
    "        side_date=0\n",
    "    [loc_title_up,loc_title_middle,loc_title_down, x_middle ]=get_size_y(json_res)\n",
    "    print(loc_title_up)\n",
    "    if [loc_title_up,loc_title_middle,loc_title_down, x_middle ]!=[(0, 0), (0, 0), (0,0), (0, 0)]:\n",
    "        fullTextAnnotation_pages = json['responses'][0]['fullTextAnnotation']['pages']\n",
    "        dict_block_by_loc_date={0:[[], []], 1:[[], []], 2:[[], []]}\n",
    "        for page in fullTextAnnotation_pages:\n",
    "            blocks = page['blocks']\n",
    "            blocks=sorted( blocks, key=lambda block:block['boundingBox']['vertices'][0]['y'] )\n",
    "            for block in blocks:\n",
    "                # 0 left\n",
    "                # 1 right\n",
    "                side=0\n",
    "                text_from_block=get_text_from_block(block)\n",
    "                words=block['paragraphs'][0]['words']\n",
    "                block_vertices = block['boundingBox']['vertices']\n",
    "                y=block_vertices[3]['y']-block_vertices[0]['y']\n",
    "                x=block_vertices[0]['x']\n",
    "                if x>=x_middle:\n",
    "                    side=1\n",
    "                if block_vertices[0]['y']<=loc_title_up[1]:\n",
    "                    ans=0\n",
    "                elif block_vertices[0]['y']<=loc_title_middle[1]:\n",
    "                    ans=1\n",
    "                else:\n",
    "                    ans=2\n",
    "                desc_words=''\n",
    "                for word in words:\n",
    "                    word_vertices=word['boundingBox']['vertices'] \n",
    "                    desc=''\n",
    "                    word1=word['symbols']\n",
    "                    for l in word1:\n",
    "                        desc+=l['text']\n",
    "                    desc_words+=desc\n",
    "                    desc_words+=' '\n",
    "                    for date in date_arr:\n",
    "                        date_split=date.split(' ')\n",
    "                        for ds in date_split:\n",
    "                            if desc==ds:\n",
    "                                dict_block_by_loc_date[ans][side].append(date)\n",
    "    if dict_block_by_loc_date[0]:\n",
    "        arr=dict_block_by_loc_date[0]\n",
    "        res_arr=arr[side_date]\n",
    "        if len(res_arr)>0:\n",
    "            return res_arr[0]\n",
    "        else:\n",
    "            res_arr=arr[abs(side_date-1)]\n",
    "            if len(res_arr)>0:\n",
    "                return res_arr[0]\n",
    "    elif dict_block_by_loc_date[2]:\n",
    "        arr=dict_block_by_loc_date[2]\n",
    "        res_arr=arr[side_date]\n",
    "        if len(res_arr)>0:\n",
    "            return res_arr[0]\n",
    "        else:\n",
    "            res_arr=arr[abs(side_date-1)]\n",
    "            if len(res_arr)>0:\n",
    "                return res_arr[0]\n",
    "    elif dict_block_by_loc_date[1]:\n",
    "        arr=dict_block_by_loc_date[1]\n",
    "        res_arr=arr[side_date]\n",
    "        if len(res_arr)>0:\n",
    "            return res_arr[0]\n",
    "        else:\n",
    "            res_arr=arr[abs(side_date-1)]\n",
    "            if len(res_arr)>0:\n",
    "                return res_arr[0]\n",
    "    return '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "073ede2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(json,json_res, arr_date):\n",
    "    [loc_title_up,loc_title_middle,loc_title_down ]=get_size_y(json_res)\n",
    "    if [loc_title_up,loc_title_middle,loc_title_down ]!=[(0, 0), (0, 0), (0,0,)]:\n",
    "        fullTextAnnotation_pages = json['responses'][0]['fullTextAnnotation']['pages']\n",
    "        word_count=get_num_words_in_json(json) \n",
    "        sorted_block_areas_num=get_big_letter_from_desc(json,word_count )\n",
    "        dict_block_by_loc_title={0:[], 1:[], 2:[]}\n",
    "        dict_mean_letter_in_block=[]\n",
    "        private_ans=0\n",
    "        for page in fullTextAnnotation_pages:\n",
    "            blocks = page['blocks']\n",
    "            if (len(blocks)<=3):\n",
    "                private_ans=2\n",
    "            for block in blocks:\n",
    "                words=block['paragraphs'][0]['words']\n",
    "                len_words=len(words)\n",
    "                if len_words<=15:\n",
    "                    mean_letter_in_block=0\n",
    "                    text=get_text_from_block(block)\n",
    "                    block_vertices = block['boundingBox']['vertices']\n",
    "                    y=block_vertices[3]['y']-block_vertices[0]['y']\n",
    "                    if block_vertices[0]['y']<=loc_title_up[1]:\n",
    "                        ans=1\n",
    "                    elif block_vertices[0]['y']<=loc_title_middle[1]:\n",
    "                        ans=2\n",
    "                    else:\n",
    "                        ans=3\n",
    "                    for word in words:\n",
    "                        desc=''\n",
    "                        word1=word['symbols']\n",
    "                        for l in word1:\n",
    "                            desc+=l['text']\n",
    "                        word_vertices=word['boundingBox']['vertices']\n",
    "                        x_values = []\n",
    "                        for vertex in block_vertices:\n",
    "                            if 'x' in vertex:\n",
    "                                x_values.append(vertex['x'])\n",
    "                        max_x = max(x_values)\n",
    "                        min_x = min(x_values)\n",
    "    \n",
    "                        y_values = []\n",
    "                        for vertex in block_vertices:\n",
    "                            if 'y' in vertex:\n",
    "                                y_values.append(vertex['y'])                         \n",
    "                        max_y = max(y_values)\n",
    "                        min_y = min(y_values)\n",
    "    \n",
    "                        block_x = max_x - min_x\n",
    "                        block_y = max_y - min_y\n",
    "\n",
    "                        num_letter = len(desc)\n",
    "                        area = block_x * block_y\n",
    "                        letter = area / num_letter\n",
    "                        mean_letter_in_block+=letter\n",
    "                    mean_letter_in_block=mean_letter_in_block/len_words\n",
    "                    \n",
    "                    text1=get_normal_text(text)\n",
    "                    text_split=text1.split(' ')\n",
    "                    flag=0\n",
    "                    c=0\n",
    "                    for t in text_split:\n",
    "                        try:\n",
    "                            float(t)\n",
    "                            c=c+1  #מספר המילים שהם מספרים\n",
    "                        except ValueError:\n",
    "                            flag=10\n",
    "                        for en in arr_entity_types:\n",
    "                            if t==en:\n",
    "                                c=c+1\n",
    "                        for date in arr_date:\n",
    "                            if t==date:\n",
    "                                c=c+1\n",
    "                    if c<(len(text_split)*0.7):\n",
    "                        if private_ans!=0:\n",
    "                            dict_mean_letter_in_block.append((text,mean_letter_in_block , private_ans))\n",
    "                        else:\n",
    "                            dict_mean_letter_in_block.append((text,mean_letter_in_block , ans))\n",
    "        sorted_dict_mean_letter_in_block = sorted(dict_mean_letter_in_block, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if sorted_dict_mean_letter_in_block!=[]:\n",
    "            min_num_of_title_letter=int((sorted_dict_mean_letter_in_block[0][1])/2)\n",
    "        else:\n",
    "            min_num_of_title_letter=0\n",
    "            \n",
    "        for i in sorted_dict_mean_letter_in_block:\n",
    "            ans=i[2]-1\n",
    "            if i[1]>=min_num_of_title_letter:\n",
    "                dict_block_by_loc_title[ans].append((i[0]))\n",
    "        print(dict_block_by_loc_title)\n",
    "        return dict_block_by_loc_title\n",
    "    # return sorted_dict_mean_letter_in_block               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ff494db-be58-4dc7-9e5f-bbb2108228ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "def check_type_binding(image_path):\n",
    "    # model_path = \"./pic_doc_ed.h5\"  # Update this with the full path to your model file\n",
    "    # loaded_model = tf.keras.models.load_model(\"binding.h5\")\n",
    "    loaded_model = tf.keras.models.load_model('./classification_models/binding.h5')\n",
    "\n",
    "    valid_images = ['jpeg', 'jpg', 'png']\n",
    "    ext = image_path.split('.')[1]\n",
    "    \n",
    "    if ext.lower() in valid_images:\n",
    "        image = Image.open(image_path)\n",
    "        resized_image = resize(np.array(image), (180, 180, 3))  # Resize to match the model's input shape\n",
    "        \n",
    "        image_features = resized_image.flatten()\n",
    "        \n",
    "        # Pad or truncate the features to match the expected number\n",
    "        if len(image_features) < (180 * 180 * 3):\n",
    "            image_features = np.pad(image_features, (0, (180 * 180 * 3) - len(image_features)), mode='constant')\n",
    "        elif len(image_features) > (180 * 180 * 3):\n",
    "            image_features = image_features[:180 * 180 * 3]\n",
    "        \n",
    "        prediction = loaded_model.predict(image_features.reshape(1, 180, 180, 3))  # Reshape to match the model's input\n",
    "        \n",
    "        # Get the class with the highest probability\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        \n",
    "        if predicted_class == 0:\n",
    "            return 'Not binding'\n",
    "        elif predicted_class == 1:\n",
    "            return 'Yad Vashem binding'\n",
    "        else:\n",
    "            return 'Another binding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25f3de8d-af85-46e8-971a-4d925cb666c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type_pic_doc_ed(image_path):\n",
    "# Load the saved KNN model\n",
    "    # loaded_model = joblib.load('logistic_model.pkl')\n",
    "    loaded_model = tf.keras.models.load_model(\"./classification_models/pic-doc-ed.h5\")\n",
    "    # Loop through the images in the folder\n",
    "    valid_images = ['jpeg', 'jpg', 'png']  # Define the valid image extensions\n",
    "    ext = image_path.split('.')[1]\n",
    "    if ext.lower() in valid_images:\n",
    "        image = Image.open(image_path)\n",
    "        # Resize the image to match the expected input size of the SVM model\n",
    "        resized_image = resize(np.array(image), (50, 50))  # Resize to (50, 50) or adjust as needed\n",
    "    \n",
    "        # Flatten and add zeros to match the expected number of features\n",
    "        image_features = resized_image.flatten()\n",
    "        if len(image_features) < 6750:\n",
    "            image_features = np.pad(image_features, (0, 6750 - len(image_features)), mode='constant')\n",
    "        elif len(image_features) > 6750:\n",
    "            image_features = image_features[:6750]\n",
    "    \n",
    "                # Apply the loaded SVM model to the image\n",
    "        prediction = loaded_model.predict(image_features.reshape(1, -1))\n",
    "    \n",
    "        if prediction == 0:\n",
    "            return 'Testimony page'\n",
    "        elif prediction == 1:\n",
    "            return 'document'\n",
    "        else:\n",
    "            return 'Image'\n",
    "    # return 'Another binding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c85cd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_Languages(data_response):\n",
    "    x = data_response['responses'][0]['fullTextAnnotation']['pages'][0]\n",
    "    if 'property' in x: \n",
    "        data_languages = data_response['responses'][0]['fullTextAnnotation']['pages'][0][\"property\"][\"detectedLanguages\"]\n",
    "        lang_map={\n",
    "            \"de\" : \"German\", \n",
    "            \"es\" :\"Spanish\",\n",
    "            \"fr\" :\"French\", \n",
    "            \"it\" :\"Italian\", \n",
    "            \"ja\" :\"Japanese\", \n",
    "            \"ko\" :\"Korean\", \n",
    "            \"ru\" : \"Russian\", \n",
    "            \"zh\" :\"Chinese (Mandarin)\", \n",
    "            \"hr\" :\"Croatian\", \n",
    "            \"nl\": \"Dutch\", \n",
    "            'pl': 'Polish',\n",
    "            'en': 'English',\n",
    "            'sk': 'Slovak',\n",
    "            'da': 'Danish',\n",
    "            'no': 'Norwegian',\n",
    "            'lt': 'Lithuanian',\n",
    "            'haw': 'Hawaiian',\n",
    "            'mt': 'Maltese',\n",
    "            'ceb': 'Cebuano',\n",
    "            'uz': 'Uzbek', \n",
    "            'ar': 'Arabic',\n",
    "            'iw':'Hebrew', \n",
    "            'yi':'Yiddish', \n",
    "            'hu':'Hungarian', \n",
    "            'lb':'Luxembourgish',  \n",
    "            'lv':'Latvian',\n",
    "            'om':'Oromo', \n",
    "            'fy':'Frisian',\n",
    "            'la':'Latin', \n",
    "            'sv' :'Swedish', \n",
    "            'af': 'Afrikaans',\n",
    "            'am': 'Amharic',\n",
    "            'az': 'Azerbaijani',\n",
    "            'be': 'Belarusian',\n",
    "            'bg': 'Bulgarian',\n",
    "            'bn': 'Bengali',\n",
    "            'bs': 'Bosnian',\n",
    "            'ca': 'Catalan',\n",
    "            'co': 'Corsican',\n",
    "            'cs': 'Czech',\n",
    "            'cy': 'Welsh',\n",
    "            'el': 'Greek',\n",
    "            'eo': 'Esperanto',\n",
    "            'et': 'Estonian',\n",
    "            'eu': 'Basque',\n",
    "            'fa': 'Persian',\n",
    "            'fi': 'Finnish',\n",
    "            'fj': 'Fijian',\n",
    "            'fo': 'Faroese',\n",
    "            'gd': 'Scottish Gaelic',\n",
    "            'gl': 'Galician',\n",
    "            'gu': 'Gujarati',\n",
    "            'ha': 'Hausa',\n",
    "            'he': 'Hebrew',\n",
    "            'hi': 'Hindi',\n",
    "            'hmn': 'Hmong',\n",
    "            'ht': 'Haitian Creole',\n",
    "            'hy': 'Armenian',\n",
    "            'id': 'Indonesian',\n",
    "            'ig': 'Igbo',\n",
    "            'is': 'Icelandic',\n",
    "            'jv': 'Javanese',\n",
    "            'ka': 'Georgian',\n",
    "            'kk': 'Kazakh',\n",
    "            'km': 'Khmer',\n",
    "            'kn': 'Kannada',\n",
    "            'ku': 'Kurdish',\n",
    "            'ky': 'Kyrgyz',\n",
    "            'lo': 'Lao',\n",
    "            'mg': 'Malagasy',\n",
    "            'mi': 'Maori',\n",
    "            'mk': 'Macedonian',\n",
    "            'ml': 'Malayalam',\n",
    "            'mn': 'Mongolian',\n",
    "            'mr': 'Marathi',\n",
    "            'ms': 'Malay',\n",
    "            'my': 'Burmese',\n",
    "            'ne': 'Nepali',\n",
    "            'ny': 'Chichewa',\n",
    "            'or': 'Odia',\n",
    "            'pa': 'Punjabi',\n",
    "            'ps': 'Pashto',\n",
    "            'pt': 'Portuguese',\n",
    "            'ro': 'Romanian',\n",
    "            'rw': 'Kinyarwanda',\n",
    "            'sd': 'Sindhi',\n",
    "            'si': 'Sinhala',\n",
    "            'sl': 'Slovenian',\n",
    "            'sm': 'Samoan',\n",
    "                'sn': 'Shona',\n",
    "            'so': 'Somali',\n",
    "            'sq': 'Albanian',\n",
    "            'sr': 'Serbian',\n",
    "            'st': 'Sesotho',\n",
    "            'su': 'Sundanese',\n",
    "            'sw': 'Swahili',\n",
    "            'ta': 'Tamil',\n",
    "            'te': 'Telugu',\n",
    "            'tg': 'Tajik',\n",
    "            'th': 'Thai',\n",
    "            'tk': 'Turkmen',\n",
    "            'tl': 'Filipino',\n",
    "            'tr': 'Turkish',\n",
    "            'tt': 'Tatar',\n",
    "            'ug': 'Uyghur',\n",
    "            'uk': 'Ukrainian',\n",
    "            'ur': 'Urdu',\n",
    "            'vi': 'Vietnamese',\n",
    "            'xh': 'Xhosa',\n",
    "            'yo': 'Yoruba',\n",
    "            'zu': 'Zulu'\n",
    "        }\n",
    "        for la in data_languages:\n",
    "            i=la['languageCode']\n",
    "            if i in lang_map:\n",
    "                la['languageCode']=lang_map[i]\n",
    "        sorted_languages = sorted(data_languages, key=lambda x: x['confidence'], reverse=True)\n",
    "        len_sorted = len(sorted_languages)\n",
    "        if len_sorted > 4:\n",
    "            len_sorted = 4\n",
    "        top_languages=[(la['languageCode'], la['confidence']) for la in sorted_languages[:len_sorted]]\n",
    "        return top_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "871d4dfe-ee1d-45fd-9392-68fc77b59e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "def write_to_csv_title(file_name):\n",
    "    with open(file_name, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Image Path', 'top_title'])\n",
    "\n",
    "def write_to_csv(data, file_name):\n",
    "    with open(file_name, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([data[0], data[1]])\n",
    "\n",
    "def read_from_csv(file_name):\n",
    "    with open(file_name, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            print(row)\n",
    "            return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e667ad3-4767-4504-a7fe-c87eabf0c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_dates_from_ocr_response(text):\n",
    "    date_formats = [\n",
    "        r'\\b\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}\\b',  # Date format: 28.05.1944 or 28/05/44\n",
    "        r'\\b\\d{1,2}\\.\\s(?:Januar|Februar|März|April|Mai|Juni|Juli|August|September|Oktober|November|Dezember)\\s\\d{2,4}\\b',  # Date format: 28. Mai 1944\n",
    "        r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s+\\d{1,2},?\\s+\\d{2,4}\\b',\n",
    "        r'\\b\\d{1,2}[./-]\\s(?:Jan\\.?|Feb\\.?|Mar\\.?|Apr\\.?|May|Jun\\.?|Jul\\.?|Aug\\.?|Sep\\.?|Oct\\.?|Nov\\.?|Dec\\.?)\\s\\d{2,4}\\b',  # Date format: 20. Jan. 1991\n",
    "        r'\\b\\d{1,2}\\s+(?:JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\\s+\\d{2,4}\\b',  # New date format\n",
    "        r'\\b\\d{4}-\\d{4}\\b'  # Date format: 2011-2012\n",
    "    ]\n",
    "\n",
    "    dates_found = []\n",
    "\n",
    "    for date_format in date_formats:\n",
    "        date_matches = re.finditer(date_format, text, re.IGNORECASE)\n",
    "        for date_match in date_matches:\n",
    "            dates_found.append(date_match.group())\n",
    "\n",
    "    return dates_found\n",
    "\n",
    "# Example usage to identify dates in '20 JULY 2020' format and '2011-2012' format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f31de51-91bf-41b0-bdf8-350e320d7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dates_new(entity_types):\n",
    "    date_finish=[]\n",
    "    for type_name, entities in entity_types.items():\n",
    "        for en in entities:\n",
    "            en_split=en.split(' ')\n",
    "            for e_split in en_split:\n",
    "                date_found=extract_dates_from_ocr_response(e_split)\n",
    "                if len(date_found)>0:\n",
    "                    flag=False\n",
    "                    date=date_found[0]\n",
    "                    for d in date_finish:\n",
    "                        if d==date:\n",
    "                            flag=True\n",
    "                    if flag==False:\n",
    "                        date_finish.append(date)\n",
    "    return(date_finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deea14ed-382b-40c1-8023-afdafe54d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_another_binding(json, json_res, date_arr):\n",
    "    [loc_title_up,loc_title_middle,loc_title_down ]=get_size_y(json_res)\n",
    "    if [loc_title_up,loc_title_middle,loc_title_down ]!=[(0, 0), (0, 0), (0,0,)]:\n",
    "        fullTextAnnotation_pages = json['responses'][0]['fullTextAnnotation']['pages']\n",
    "        word_count=get_num_words_in_json(json) \n",
    "        sorted_block_areas_num=get_big_letter_from_desc(json,word_count )\n",
    "        dict_block_by_loc_title={0:[], 1:[], 2:[]}\n",
    "        dict_mean_letter_in_block=[]\n",
    "        private_ans=0\n",
    "        for page in fullTextAnnotation_pages:\n",
    "            blocks = page['blocks']\n",
    "            if (len(blocks)<=3):\n",
    "                private_ans=2\n",
    "            for block in blocks:\n",
    "                words=block['paragraphs'][0]['words']\n",
    "                len_words=len(words)\n",
    "                if len_words<=15:\n",
    "                    mean_letter_in_block=0\n",
    "                    text=get_text_from_block(block)\n",
    "                    block_vertices = block['boundingBox']['vertices']\n",
    "                    y=block_vertices[3]['y']-block_vertices[0]['y']\n",
    "                    if block_vertices[0]['y']<=loc_title_up[1]:\n",
    "                        ans=1\n",
    "                    elif block_vertices[0]['y']<=loc_title_middle[1]:\n",
    "                        ans=2\n",
    "                    else:\n",
    "                        ans=3\n",
    "                    for word in words:\n",
    "                        desc=''\n",
    "                        word1=word['symbols']\n",
    "                        for l in word1:\n",
    "                            desc+=l['text']\n",
    "                        word_vertices=word['boundingBox']['vertices']\n",
    "                        x_values = []\n",
    "                        for vertex in block_vertices:\n",
    "                            if 'x' in vertex:\n",
    "                                x_values.append(vertex['x'])\n",
    "                        max_x = max(x_values)\n",
    "                        min_x = min(x_values)\n",
    "    \n",
    "                        y_values = []\n",
    "                        for vertex in block_vertices:\n",
    "                            if 'y' in vertex:\n",
    "                                y_values.append(vertex['y'])                         \n",
    "                        max_y = max(y_values)\n",
    "                        min_y = min(y_values)\n",
    "    \n",
    "                        block_x = max_x - min_x\n",
    "                        block_y = max_y - min_y\n",
    "\n",
    "                        num_letter = len(desc)\n",
    "                        area = block_x * block_y\n",
    "                        letter = area / num_letter\n",
    "                        mean_letter_in_block+=letter\n",
    "                    mean_letter_in_block=mean_letter_in_block/len_words\n",
    "                    \n",
    "                    text1=get_normal_text(text)\n",
    "                    text_split=text1.split(' ')\n",
    "                    flag=0\n",
    "                    c=0\n",
    "                    for t in text_split:\n",
    "                        try:\n",
    "                            float(t)\n",
    "                            c=c+1  #מספר המילים שהם מספרים\n",
    "                        except ValueError:\n",
    "                            flag=10\n",
    "                        for en in arr_entity_types:\n",
    "                            if t==en:\n",
    "                                c=c+1\n",
    "                        for date in arr_date:\n",
    "                            if t==date:\n",
    "                                c=c+1\n",
    "                    if c<(len(text_split)*0.7):\n",
    "                        if private_ans!=0:\n",
    "                            dict_mean_letter_in_block.append((text,mean_letter_in_block , private_ans))\n",
    "                        else:\n",
    "                            dict_mean_letter_in_block.append((text,mean_letter_in_block , ans))\n",
    "    sorted_dict_mean_letter_in_block = sorted(dict_mean_letter_in_block, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_dict_mean_letter_in_block[2:]\n",
    "        # if sorted_dict_mean_letter_in_block!=[]:\n",
    "        #     min_num_of_title_letter=int((sorted_dict_mean_letter_in_block[0][1])/2)\n",
    "        # else:\n",
    "        #     min_num_of_title_letter=0\n",
    "            \n",
    "        # for i in sorted_dict_mean_letter_in_block:\n",
    "        #     ans=i[2]-1\n",
    "        #     if i[1]>=min_num_of_title_letter:\n",
    "        #         dict_block_by_loc_title[ans].append((i[0]))\n",
    "        # print(dict_block_by_loc_title)\n",
    "        # return dict_block_by_loc_title\n",
    "    # return sorted_dict_mean_letter_in_block               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "397a0826-a122-4aa9-bb48-9c7077ef4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_Testimony_page(json, format_json, json_res):\n",
    "    date_arr=[]\n",
    "    date=''\n",
    "    date1=extract_dates_from_ocr_response(format_json['full_desc_with_verices'][0]['text'])\n",
    "    if len(date1)>0:\n",
    "        date_arr=date1\n",
    "    entity_types=format_json['entity_types']\n",
    "    if 'DATE' in entity_types:\n",
    "        date2=entity_types['DATE']\n",
    "        for date in date2:\n",
    "            if date not in date_arr:\n",
    "                date_arr.append(date)\n",
    "    if 'CARDINAL' in entity_types:\n",
    "        for car in entity_types['CARDINAL']:\n",
    "            d=extract_dates_from_ocr_response(car)\n",
    "            if len(d)>0:\n",
    "                if d[0] not in date_arr:\n",
    "                    date_arr.append(d[0])\n",
    "    date3=append_dates_new(entity_types)\n",
    "    if len(date3)>0:\n",
    "        for date in date3:\n",
    "            if date not in date_arr:\n",
    "                date_arr.append(date)\n",
    "    if len(date_arr)>0:\n",
    "        date_arr=list(set(date_arr))\n",
    "        res_date =  get_date_Testimony(json_res, json, date_arr, format_json)\n",
    "        if len(res_date)>0:\n",
    "            date= res_date  \n",
    "    return date_arr, date   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4f434b5-377c-4477-a3ff-daf308d079b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_document( json, format_json, json_res):\n",
    "    date_arr=[]\n",
    "    date=''\n",
    "    date1=extract_dates_from_ocr_response(format_json['full_desc_with_verices'][0]['text'])\n",
    "    if len(date1)>0:\n",
    "        date_arr=date1\n",
    "    entity_types=format_json['entity_types']\n",
    "    if 'DATE' in entity_types:\n",
    "        date2=entity_types['DATE']\n",
    "        for date in date2:\n",
    "            d=extract_dates_from_ocr_response(date)\n",
    "            if len(d)>0:\n",
    "                if d[0] not in date_arr:\n",
    "                    date_arr.append(d[0])\n",
    "    if 'CARDINAL' in entity_types:\n",
    "        for car in entity_types['CARDINAL']:\n",
    "            d=extract_dates_from_ocr_response(car)\n",
    "            if len(d)>0:\n",
    "                if d[0] not in date_arr:\n",
    "                    date_arr.append(d[0])\n",
    "    date3=append_dates_new(entity_types)\n",
    "    if len(date3)>0:\n",
    "        for date in date3:\n",
    "            d=extract_dates_from_ocr_response(date)\n",
    "            if len(d)>0:\n",
    "                if d[0] not in date_arr:\n",
    "                    date_arr.append(d[0])\n",
    "    if len(date_arr)>0:\n",
    "        date_arr=list(set(date_arr))\n",
    "        res_date =  get_date(json_res, json, date_arr, format_json)\n",
    "        if len(res_date)>0:\n",
    "            date= res_date  \n",
    "    return date_arr, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f2719-6f1a-4658-8ce1-d3ab70a34d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8817f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_format_json(img_path, json, format_json):\n",
    "    page_type=format_json['page type']\n",
    "    format_json['data_Languages'] = get_data_Languages(json)\n",
    "    json_res = create_json_get_by_y(json)\n",
    "    if json_res:\n",
    "        format_json['full_desc_with_verices'] = json_res.get('full_desc_with_verices', [])\n",
    "        format_json['words'] = json_res.get('words', [])\n",
    "        text_ = format_json['full_desc_with_verices'][0].get('text', '')\n",
    "        text_ = ' '.join(text_.split(\"\\n\")) if text_ else ''\n",
    "        format_json['full_desc_with_verices'][0]['text'] = text_\n",
    "        entity_types = get_finds_dates_places_persons(text_)\n",
    "        format_json['entity_types']=entity_types \n",
    "        if page_type=='another binding':\n",
    "            print(\"ddddddddddddddddddd\")\n",
    "            title = get_title_another_binding(json, json_res, date_arr)\n",
    "            if title:\n",
    "                print(\"title\", title)\n",
    "                format_json['top_title'] = title\n",
    "                return format_json\n",
    "        if page_type=='yad vashem binding':\n",
    "            print(\"ddddddddddddddddddd\")\n",
    "            format_json['top_title']=['YAD VASHEM ARCHIVES', 'ארכיון יד ושם']\n",
    "            return format_json\n",
    "        if page_type=='Testimony page':\n",
    "            format_json['top_title']=['page of Testimony', 'דף עד']\n",
    "            date_arr, date=get_date_from_Testimony_page(json, format_json,json_res )\n",
    "            print(date)\n",
    "            format_json['Production date']=date\n",
    "            return format_json\n",
    "        if  page_type=='Testimony page including a photograph':\n",
    "            format_json['top_title']=['page of Testimony', 'דף עד']\n",
    "            date_arr, date=get_date_from_Testimony_page(json, format_json,json_res )\n",
    "            print(date)\n",
    "            format_json['Production date']=date\n",
    "            return format_json\n",
    "        if page_type=='document':\n",
    "            date_arr, date=get_date_from_document(json, format_json,json_res )\n",
    "            format_json['Production date']=date\n",
    "            dict_block_by_loc_title = get_title(json, json_res, date_arr)\n",
    "            if dict_block_by_loc_title:\n",
    "                title = dict_block_by_loc_title[0]\n",
    "                if len(title)>3:\n",
    "                    title=title[:3]\n",
    "                format_json['top_title'] = title\n",
    "                return format_json\n",
    "    return format_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d401cd68-4fa9-4ca0-bb62-20487a01943c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f3f89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_file(json_filename):\n",
    "    try:\n",
    "        with open(json_filename, 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f99ce010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_folders(main_folder):\n",
    "    valid_images = ['.json']  # Define the valid image extensions\n",
    "    imgs_dict = {}\n",
    "    regular_imgs=[]\n",
    "    pictures=[]\n",
    "    to_remove=[]\n",
    "    for folder in os.listdir(main_folder):\n",
    "        folder_path = os.path.join(main_folder, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for image_file in os.listdir(folder_path):\n",
    "            ext = os.path.splitext(image_file)[1]\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            if ext.lower() not in valid_images:\n",
    "                type_pred=check_type_page(image_path)\n",
    "                if type_pred=='picture':\n",
    "                    pictures.append(image_path)\n",
    "                else:\n",
    "                    regular_imgs.append(image_path)\n",
    "            else:\n",
    "                json=read_json_file(image_path)\n",
    "                if json!=None:\n",
    "                    imgs_dict[image_path]=json\n",
    "                else:\n",
    "                    to_remove.append(image_path.split('.')[0])\n",
    "    regular_imgs_2=[]\n",
    "    for i in regular_imgs:\n",
    "        ii=i.split('.')[0]\n",
    "        if ii not in to_remove:\n",
    "            regular_imgs_2.append(i)\n",
    "    return imgs_dict, regular_imgs_2, pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81dfbf42-038b-4aba-9aa5-0bb05f138942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_for_json(json_filename, json_object):\n",
    "    with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(json_object, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b0191d4-06be-4424-9e8b-66a9a2fcf206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ed(class_names, img_height, img_width, pic_path, loaded_model):\n",
    "    img = tf.keras.utils.load_img(pic_path, target_size=(img_height, img_width))\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    predictions = loaded_model.predict(img_array)\n",
    "    \n",
    "    return class_names[np.argmax(predictions[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99905778-9a71-4c21-8ea7-af236b89b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_type_ed(image):\n",
    "    loaded_model = tf.keras.models.load_model(\"./classification_models/ed.h5\")\n",
    "    class_names = ['ed', 'edp']\n",
    "    img_height = 180\n",
    "    img_width = 180\n",
    "    if image.endswith(('.jpg', '.JPG', '.png', '.gif')):\n",
    "        pred = predict_ed(class_names, img_height, img_width, image, loaded_model)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09c4a537-a2bc-41b9-9383-44e88aa2d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pic_doc_ed(class_names, img_height, img_width, pic_path, loaded_model):\n",
    "    img = tf.keras.utils.load_img(pic_path, target_size=(img_height, img_width))\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    predictions = loaded_model.predict(img_array)\n",
    "    \n",
    "    return class_names[np.argmax(predictions[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "976c4c9a-fd32-4720-8721-147ff8734416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_type_pic_doc_ed(image):\n",
    "    loaded_model = tf.keras.models.load_model(\"./classification_models/pic-doc-ed.h5\")\n",
    "    class_names = ['pictures', 'documents', 'ed']\n",
    "    img_height = 180\n",
    "    img_width = 180\n",
    "    if image.endswith(('.jpg', '.JPG', '.png', '.gif')):\n",
    "        pred = predict_pic_doc_ed(class_names, img_height, img_width, image, loaded_model)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bdf4243-92aa-4684-9bb2-f0b6fe78cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binding(class_names, img_height, img_width, pic_path, loaded_model):\n",
    "    img = tf.keras.utils.load_img(pic_path, target_size=(img_height, img_width))\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    predictions = loaded_model.predict(img_array)\n",
    "    return class_names[np.argmax(predictions[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d6f0388-9d0a-47b5-a8f5-3b996884ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_type_binding(image):\n",
    "    loaded_model = tf.keras.models.load_model(\"./classification_models/binding.h5\")\n",
    "    class_names = ['not binding', 'yad vashem binding', 'another binding']\n",
    "    img_height = 180\n",
    "    img_width = 180\n",
    "    if image.endswith(('.jpg', '.JPG', '.png', '.gif')):\n",
    "        pred = predict_binding(class_names, img_height, img_width, image, loaded_model)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf209f3f-3014-467a-9366-0b731add451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type_page(image_path):\n",
    "    pred_pic_doc_ed=predict_type_pic_doc_ed(image_path)\n",
    "    if pred_pic_doc_ed=='pictures':\n",
    "        return 'picture'\n",
    "    elif pred_pic_doc_ed=='documents':\n",
    "        pred_bind=predict_type_binding(image_path)\n",
    "        if pred_bind=='not binding':\n",
    "            return 'document'\n",
    "        else:\n",
    "            pred_bind\n",
    "    else:\n",
    "        pred_ed=predict_type_ed(image_path)\n",
    "        if pred_ed=='ed':\n",
    "            return 'Testimony page'\n",
    "        else :\n",
    "            return 'Testimony page including a photograph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba938f22-6e9e-4713-a1b1-e0fffa95b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_sql(json_data):\n",
    "    image_name=json_data.get(\"image_path\", '')\n",
    "    full_desc_with_verices=json_data.get(\"full_desc_with_verices\", '')\n",
    "    lang_text_1=''\n",
    "    lang_text_1+=str(full_desc_with_verices[0])\n",
    "    lang_text_1+=','\n",
    "    lang_text_1+=str(full_desc_with_verices[1])\n",
    "    full_desc_with_verices=lang_text_1\n",
    "    words=json_data.get(\"words\", '')\n",
    "    flat_data = [item for sublist in words for item in sublist]\n",
    "    long_string = ', '.join(map(str, flat_data))\n",
    "    words=long_string\n",
    "    top_title=json_data.get(\"top_title\", '')\n",
    "    format_json_path=json_data.get(\"format_json_path\", '')\n",
    "    json_path=json_data.get(\"json_path\", '')\n",
    "    date=json_data.get(\"Production date\", '')\n",
    "    if date==[]:\n",
    "        date=''\n",
    "    type_page=json_data.get(\"type_page\", '')\n",
    "    entity_types=str(json_data.get(\"entity_types\", ''))\n",
    "# Establish connection to the SQL Server database\n",
    "    connection_string = 'DRIVER={SQL Server};SERVER=DESKTOP-E0FAPSB\\SQLEXPRESS;DATABASE=Practicom-DB-format_json;Trusted_Connection=yes;'\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Insert data into the Master table\n",
    "        cursor.execute(\"INSERT INTO Master (IMG_NAME, FULL_DESC_WITH_VERTICES, WORDS, DATE, JSON_PATH, FORMAT_JSON_PATH, TYPE_PAGE, ENTITY_YTPES) VALUES (?, ?, ?, ?, ?, ?, ?)\", \n",
    "                           (image_name, full_desc_with_verices, words, date, json_path, format_json_path, type_page, entity_types))\n",
    "        connection.commit()\n",
    "    \n",
    "        # Get the generated DOC_ID\n",
    "        cursor.execute(\"SELECT @@IDENTITY AS LastIdentity\")\n",
    "        doc_id = cursor.fetchone().LastIdentity\n",
    "        # Insert data into the Title table with the correct DOC_ID\n",
    "        for title_text in top_title:\n",
    "            cursor.execute(\"INSERT INTO Title (DOC_ID, TITLE_TEXT) VALUES (?, ?)\", (doc_id, title_text))\n",
    "        connection.commit()\n",
    "    \n",
    "        # Insert data into the Data_Languages table\n",
    "        data_languages = json_data.get(\"data_Languages\", [])\n",
    "        for language, confidence in data_languages:\n",
    "            cursor.execute(\"INSERT INTO Data_Languages (DOC_ID, LANGUAGE, CONFIDENCE) VALUES (?, ?, ?)\", (doc_id, language, confidence))\n",
    "        connection.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during the database operations\n",
    "        print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49f65d2f-766e-4bd0-8f8a-9941a46bc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_sql_pictures(picture):\n",
    "    connection_string = 'DRIVER={SQL Server};SERVER=DESKTOP-E0FAPSB\\SQLEXPRESS;DATABASE=Practicom-DB-format_json;Trusted_Connection=yes;'\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Insert data into the Master table\n",
    "        cursor.execute(\"INSERT INTO Pictures (IMG_PATH) VALUES (?)\", \n",
    "                           (picture))\n",
    "        connection.commit()\n",
    "    except Exception as e:\n",
    "# Handle any exceptions that may occur during the database operations\n",
    "        print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "890c0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "def second_steps_for_all_images(data):\n",
    "    new_json_dict={}\n",
    "    regular_imgs=[]\n",
    "    imgs_dict, regular_imgs, pictures=process_images_in_folders(data)\n",
    "    img=''\n",
    "    print(len(imgs_dict), len(regular_imgs), len(pictures))\n",
    "    for picture in pictures:\n",
    "        print(\"pic\")\n",
    "        # write_to_sql_pictures(picture)\n",
    "    for img_path, json in imgs_dict.items():\n",
    "        format_json={}\n",
    "        format_json['page type']=check_type_page(img_path)\n",
    "        format_json=create_format_json(img_path, json, format_json)\n",
    "        format_json['json_path']=img_path\n",
    "        img=img_path.split('.')[0]\n",
    "        new_json_dict[img]=format_json\n",
    "    for img_path in regular_imgs:\n",
    "        format_json['image_path']=img_path\n",
    "        img1=img_path.split('.')[0]\n",
    "        format_json=new_json_dict[img1]\n",
    "        format_json['format_json_path']=img1+'_format.json'\n",
    "        # write_to_sql(format_json)\n",
    "        # write_for_json(format_json['format_json_path'], format_json)\n",
    "        # print(img_path, format_json.get('Production date', 0), '  :::  ', format_json.get('top_title', ''))\n",
    "        # print(\"img_path\", img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4140e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d15ff864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001852947F640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001852947F640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018529560040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018529560040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 16 8\n",
      "pic\n",
      "pic\n",
      "pic\n",
      "pic\n",
      "pic\n",
      "pic\n",
      "pic\n",
      "pic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 520.75)\n",
      "וואָס\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 191.0)\n",
      "1.1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 177.5)\n",
      "44-4.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 179.75)\n",
      "44-4.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 187.25)\n",
      "1.5.40 - 20.8.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 198.0)\n",
      "1906\n",
      "(50, 215.25)\n",
      "29.10.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(403, 893.5)\n",
      "25 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(329, 873.75)\n",
      "2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 217.5)\n",
      "10.11.1988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119, 482.5)\n",
      "12.2.1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 127.0)\n",
      "30.8.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 201.25)\n",
      "10-30781-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 223.25)\n",
      "Monday\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 153.0)\n",
      "8. april 1946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 931.75)\n",
      "3.4.1955\n",
      "(23, 177.75)\n",
      "23531\n"
     ]
    }
   ],
   "source": [
    "data='data_practucom/test'\n",
    "second_steps_for_all_images(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e4ddb-6536-4646-9898-89a413574ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588267f-7eb9-4766-8c6d-35dea2e71060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
