{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1df835d-48d6-4c18-a056-e8abde667547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5b86b6-6e89-46ee-9d0f-17ddb4fa4b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf3184c-e889-4f79-80e4-103c95cc6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'C:\\Python310\\python.exe -m pip install --upgrade pip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a67b48f-a483-44c6-9603-ea57abfb13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e42bcb6-2e97-42ec-b1a9-2841e91b75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-image\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98cff1f-d05b-444c-8fcd-951c34060d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345cc99f-c933-449c-a2bc-202f813522ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade --force-reinstall tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dbeb387-cf7c-42ef-b4e3-680d700728ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb7bcdc9-6747-4eee-962f-a84d5338991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyodbc in c:\\python310\\lib\\site-packages (5.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecbb195c-cefa-4370-a1f7-908f2dd91415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import os, os.path\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "import glob\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import ast\n",
    "import numpy as np\n",
    "import joblib\n",
    "import ast\n",
    "from skimage.transform import resize\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import pyodbc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb5df2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4725b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_trf\n",
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6528f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3f9f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e300692b-1f6b-45e2-96dd-9e208cc9d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_text(i):\n",
    "    if i:\n",
    "    # Check and replace '.' if it exists in the string\n",
    "        if '.' in i:\n",
    "            i = i.replace('.', '')\n",
    "    \n",
    "        # Check and replace '/' if it exists in the string\n",
    "        if '/' in i:\n",
    "            i = i.replace('/', '')\n",
    "    \n",
    "        # Check and replace ',' if it exists in the string\n",
    "        if ',' in i:\n",
    "            i = i.replace(',', '')\n",
    "    \n",
    "        # Check and replace ':' if it exists in the string\n",
    "        if ':' in i:\n",
    "            i = i.replace(':', '')\n",
    "    \n",
    "        # Check and replace ')' if it exists in the string\n",
    "        if ')' in i:\n",
    "            i = i.replace(')', '')\n",
    "\n",
    "    # Check and replace '(' if it exists in the string\n",
    "        if '(' in i:\n",
    "            i = i.replace('(', '')\n",
    "\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebe72cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finds_dates_places_persons(desc):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(desc)\n",
    "    entity_types = {}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        type_name = ent.label_\n",
    "        entity = ent.text\n",
    "\n",
    "        if type_name not in entity_types:\n",
    "            entity_types[type_name] = []\n",
    "\n",
    "        entity_types[type_name].append(entity)\n",
    "    target_word=nlp(\"study\")\n",
    "    return entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "699835b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words_in_json(json):\n",
    "    textAnnotations = json['responses'][0]['textAnnotations']\n",
    "    word_count=0\n",
    "    for word in textAnnotations:\n",
    "        desc = word['description']\n",
    "        word_count+=1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1ab72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_with_vertices(data_response):\n",
    "    textAnnotations=data_response['responses'][0][\"textAnnotations\"]\n",
    "    text=data_response['responses'][0]['fullTextAnnotation']['text']\n",
    "    arr_res=[]\n",
    "    arr_res.append(({'text':text},textAnnotations[0]['boundingPoly'] ))\n",
    "    for idx, i in enumerate(textAnnotations, start=1):\n",
    "        ver=i[\"boundingPoly\"]\n",
    "        desc=i['description']\n",
    "        desc=desc.split(\"\\n\")\n",
    "        list1=(ver,desc)\n",
    "        arr_res.append(list1)\n",
    "    return [arr_res[1:], arr_res[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a576acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_get_by_y(json):\n",
    "    [words,full_desc_with_verices] =get_all_words_with_vertices(json)\n",
    "    json_res={}\n",
    "    json_res[\"full_desc_with_verices\"]=full_desc_with_verices\n",
    "    json_res['words']=words\n",
    "    return json_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a7636d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_y(json):\n",
    "    vertices = json[\"full_desc_with_verices\"][1][\"vertices\"]\n",
    "    y_start = vertices[0].get(\"y\", 0)  # default value 0 if 'y' key is not found in the dictionary\n",
    "    y_end = vertices[3].get(\"y\", 0)\n",
    "    \n",
    "    if \"y\" in vertices[0] and \"y\" in vertices[3]:\n",
    "        range_ = vertices[3][\"y\"] - vertices[0][\"y\"]\n",
    "    else:\n",
    "        range_ = 0  # or set a default value if 'y' key is missing in some vertices\n",
    "    # y_start= json[\"full_desc_with_verices\"][1][\"vertices\"][0][\"y\"]\n",
    "    # y_end= json[\"full_desc_with_verices\"][1][\"vertices\"][3][\"y\"]\n",
    "\n",
    "    # range_= json[\"full_desc_with_verices\"][1][\"vertices\"][3][\"y\"]-json[\"full_desc_with_verices\"][1][\"vertices\"][0][\"y\"]\n",
    "    if range_ != 0:\n",
    "        loc_title_up= (y_start,y_start+range_/4)\n",
    "        loc_title_middle=(y_start+range_/4,y_start+(range_/3)*2)\n",
    "        loc_title_down=(y_start+(range_/3)*2,y_start+range_)\n",
    "        return [loc_title_up,loc_title_middle,loc_title_down ]\n",
    "    else:\n",
    "        return [(0, 0), (0, 0), (0,0,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72c3d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_block(block):\n",
    "    text=''\n",
    "    words=block['paragraphs'][0]['words']\n",
    "    for w in words:\n",
    "        word=w['symbols']\n",
    "        for l in word:\n",
    "            text+=l['text']\n",
    "        text+=' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "730f057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_big_letter_from_desc(json, words_count):\n",
    "    text_annotations = json.get('responses', [{}])[0].get('textAnnotations', [])\n",
    "    \n",
    "    block_areas_num = []\n",
    "    \n",
    "    for word in text_annotations[1:]:\n",
    "        desc = word.get('description', '')\n",
    "        block_vertices = word.get('boundingPoly', {}).get('vertices', [])\n",
    "        x_values = [vertex.get('x', 0) for vertex in block_vertices if 'x' in vertex]\n",
    "        y_values = [vertex.get('y', 0) for vertex in block_vertices if 'y' in vertex]\n",
    "        \n",
    "        if len(x_values) > 0 and len(y_values) > 0:\n",
    "            max_x = max(x_values)\n",
    "            min_x = min(x_values)\n",
    "\n",
    "            max_y = max(y_values)\n",
    "            min_y = min(y_values)\n",
    "\n",
    "            block_x = max_x - min_x\n",
    "            block_y = max_y - min_y\n",
    "\n",
    "            num_letter = len(desc)\n",
    "            area = block_x * block_y\n",
    "            letter = area / num_letter\n",
    "            \n",
    "            if desc not in ['(', ')', '-', '[', ']', ',', '.']:\n",
    "                block_areas_num.append((desc, letter))\n",
    "    \n",
    "    sorted_block_areas_num = sorted(block_areas_num, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_block_areas_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9405a3c-765f-4f33-be32-2ebfaa678d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # date_found=extract_dates_from_ocr_response(desc)\n",
    "# #                     if date_found!=[]:\n",
    "# #                         print(\"wwwwwwwwwwwwwwwwwwwwwww\", date_found)\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "#                     if flag==True:\n",
    "#                         print('desc_words', desc_words)\n",
    "#                         desc_words_split=desc_words.split(' ')\n",
    "#                         if  'date' in desc_words_split:\n",
    "#                             desc_words_split.remove('date')\n",
    "#                         if  'Date' in desc_words_split:\n",
    "#                             desc_words_split.remove('Date')\n",
    "#                         desc_good=''\n",
    "#                         for dd in  desc_words_split:\n",
    "#                             desc_good+=dd\n",
    "#                         more_date_1=extract_dates_from_ocr_response(desc_good)\n",
    "#                         if more_date_1!=[]:\n",
    "#                             more_date.append(more_date_1)\n",
    "#                             print(\"qqqqqqqqqqqqqqqqq \", more_date_1)\n",
    "#                         if more_date!=[]:\n",
    "#                             res_date=more_date\n",
    "#                     else:\n",
    "#                         date_full=extract_dates_from_ocr_response(desc_good)\n",
    "#                         if date_full!=[]:\n",
    "#                             date_arr.append(date_full[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e26dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_try(json, date_arr, cardinal_maiby_date, flag):\n",
    "    more_date=[]\n",
    "    json_res=create_json_get_by_y(json)\n",
    "    res_date=''\n",
    "    [loc_title_up,loc_title_middle,loc_title_down ]=get_size_y(json_res)\n",
    "    if [loc_title_up,loc_title_middle,loc_title_down ]!=[(0, 0), (0, 0), (0,0,)]:\n",
    "        fullTextAnnotation_pages = json['responses'][0]['fullTextAnnotation']['pages']\n",
    "        dict_block_by_loc_date={0:[], 1:[], 2:[]}\n",
    "        for page in fullTextAnnotation_pages:\n",
    "            blocks = page['blocks']\n",
    "            for block in blocks:\n",
    "                text_from_block=get_text_from_block(block)\n",
    "                words=block['paragraphs'][0]['words']\n",
    "                block_vertices = block['boundingBox']['vertices']\n",
    "                y=block_vertices[3]['y']-block_vertices[0]['y']\n",
    "                if block_vertices[0]['y']<=loc_title_up[1]:\n",
    "                    ans=0\n",
    "                elif block_vertices[0]['y']<=loc_title_middle[1]:\n",
    "                    ans=1\n",
    "                else:\n",
    "                    ans=2\n",
    "                desc_words=''\n",
    "                for word in words:\n",
    "                    word_vertices=word['boundingBox']['vertices']   \n",
    "                    desc=''\n",
    "                    word1=word['symbols']\n",
    "                    for l in word1:\n",
    "                        desc+=l['text']\n",
    "                    desc_words+=desc\n",
    "                    desc_words+=' '\n",
    "                    for date in date_arr:\n",
    "                        date_split=date.split(' ')\n",
    "                        for ds in date_split:\n",
    "                            if desc==ds:\n",
    "                                flag1=False\n",
    "                                date_ans= dict_block_by_loc_date[ans]\n",
    "                                for d_ans in date_ans:\n",
    "                                    if date==d_ans[0] :\n",
    "                                        flag1=True\n",
    "                                if flag1==False:\n",
    "                                    dict_block_by_loc_date[ans].append((date, ans))\n",
    "    if flag==False:   \n",
    "        if dict_block_by_loc_date[0]:\n",
    "            res_date=dict_block_by_loc_date[0]\n",
    "        elif dict_block_by_loc_date[2]:\n",
    "            res_date=dict_block_by_loc_date[2]\n",
    "        elif dict_block_by_loc_date[1]:\n",
    "            res_date=dict_block_by_loc_date[1]\n",
    "    # print(date_arr, ' ', dict_block_by_loc_date)\n",
    "    return res_date   \n",
    "                                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602b596-3949-4184-b2e9-9153657df367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc41bfb4-62ba-482f-90fa-9bf15d8141b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073ede2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_try(json,json_res, arr_entity_types):\n",
    "    [loc_title_up,loc_title_middle,loc_title_down ]=get_size_y(json_res)\n",
    "    if [loc_title_up,loc_title_middle,loc_title_down ]!=[(0, 0), (0, 0), (0,0,)]:\n",
    "        fullTextAnnotation_pages = json['responses'][0]['fullTextAnnotation']['pages']\n",
    "        word_count=get_num_words_in_json(json) \n",
    "        sorted_block_areas_num=get_big_letter_from_desc(json,word_count )\n",
    "        dict_block_by_loc_title={0:[], 1:[], 2:[]}\n",
    "        dict_mean_letter_in_block=[]\n",
    "        private_ans=0\n",
    "        for page in fullTextAnnotation_pages:\n",
    "            blocks = page['blocks']\n",
    "            if (len(blocks)<=3):\n",
    "                private_ans=2\n",
    "            for block in blocks:\n",
    "                words=block['paragraphs'][0]['words']\n",
    "                len_words=len(words)\n",
    "                if len_words<=15:\n",
    "                    mean_letter_in_block=0\n",
    "                    text=get_text_from_block(block)\n",
    "                    block_vertices = block['boundingBox']['vertices']\n",
    "                    y=block_vertices[3]['y']-block_vertices[0]['y']\n",
    "                    if block_vertices[0]['y']<=loc_title_up[1]:\n",
    "                        ans=1\n",
    "                    elif block_vertices[0]['y']<=loc_title_middle[1]:\n",
    "                        ans=2\n",
    "                    else:\n",
    "                        ans=3\n",
    "                    for word in words:\n",
    "                        desc=''\n",
    "                        word1=word['symbols']\n",
    "                        for l in word1:\n",
    "                            desc+=l['text']\n",
    "                        word_vertices=word['boundingBox']['vertices']\n",
    "                        x_values = []\n",
    "                        for vertex in block_vertices:\n",
    "                            if 'x' in vertex:\n",
    "                                x_values.append(vertex['x'])\n",
    "                        max_x = max(x_values)\n",
    "                        min_x = min(x_values)\n",
    "    \n",
    "                        y_values = []\n",
    "                        for vertex in block_vertices:\n",
    "                            if 'y' in vertex:\n",
    "                                y_values.append(vertex['y'])                         \n",
    "                        max_y = max(y_values)\n",
    "                        min_y = min(y_values)\n",
    "    \n",
    "                        block_x = max_x - min_x\n",
    "                        block_y = max_y - min_y\n",
    "\n",
    "                        num_letter = len(desc)\n",
    "                        area = block_x * block_y\n",
    "                        letter = area / num_letter\n",
    "                        mean_letter_in_block+=letter\n",
    "                    mean_letter_in_block=mean_letter_in_block/len_words\n",
    "                    \n",
    "                    text1=get_normal_text(text)\n",
    "                    text_split=text1.split(' ')\n",
    "                    flag=0\n",
    "                    c=0\n",
    "                    for t in text_split:\n",
    "                        try:\n",
    "                            float(t)\n",
    "                            c=c+1  #מספר המילים שהם מספרים\n",
    "                        except ValueError:\n",
    "                            flag=10\n",
    "                        for en in arr_entity_types:\n",
    "                            if t==en:\n",
    "                                c=c+1\n",
    "                    if c<(len(text_split)*0.5):\n",
    "                        if private_ans!=0:\n",
    "                            dict_mean_letter_in_block.append((text,mean_letter_in_block , private_ans))\n",
    "                        else:\n",
    "                            dict_mean_letter_in_block.append((text,mean_letter_in_block , ans))\n",
    "        sorted_dict_mean_letter_in_block = sorted(dict_mean_letter_in_block, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if sorted_dict_mean_letter_in_block!=[]:\n",
    "            min_num_of_title_letter=int((sorted_dict_mean_letter_in_block[0][1])/2)\n",
    "        else:\n",
    "            min_num_of_title_letter=0\n",
    "            \n",
    "        for i in sorted_dict_mean_letter_in_block:\n",
    "            ans=i[2]-1\n",
    "            if i[1]>=min_num_of_title_letter:\n",
    "                dict_block_by_loc_title[ans].append((i[0]))\n",
    "        return dict_block_by_loc_title\n",
    "    # return sorted_dict_mean_letter_in_block               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c332547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_type_binding(image_path):\n",
    "# # Load the saved KNN model\n",
    "#     # loaded_model = joblib.load('logistic_model.pkl')\n",
    "#     loaded_model = tf.keras.models.load_model(\"binding.h5\")\n",
    "#     # Loop through the images in the folder\n",
    "#     valid_images = ['jpeg', 'jpg', 'png']  # Define the valid image extensions\n",
    "#     ext = image_path.split('.')[1]\n",
    "#     if ext.lower() in valid_images:\n",
    "#         image = Image.open(image_path)\n",
    "#         # Resize the image to match the expected input size of the SVM model\n",
    "#         resized_image = resize(np.array(image), (50, 50))  # Resize to (50, 50) or adjust as needed\n",
    "    \n",
    "#         # Flatten and add zeros to match the expected number of features\n",
    "#         image_features = resized_image.flatten()\n",
    "#         if len(image_features) < 6750:\n",
    "#             image_features = np.pad(image_features, (0, 6750 - len(image_features)), mode='constant')\n",
    "#         elif len(image_features) > 6750:\n",
    "#             image_features = image_features[:6750]\n",
    "    \n",
    "#                 # Apply the loaded SVM model to the image\n",
    "#         prediction = loaded_model.predict(image_features.reshape(1, -1))\n",
    "    \n",
    "#         if prediction == 0:\n",
    "#             return 'Not binding'\n",
    "#         elif prediction == 1:\n",
    "#             return ' Yad Vashem binding'\n",
    "#         else:\n",
    "#             return 'Another binding'\n",
    "#     # return 'Another binding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ff494db-be58-4dc7-9e5f-bbb2108228ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "def check_type_binding(image_path):\n",
    "    # model_path = \"./pic_doc_ed.h5\"  # Update this with the full path to your model file\n",
    "    # loaded_model = tf.keras.models.load_model(\"binding.h5\")\n",
    "    loaded_model = tf.keras.models.load_model('binding.h5')\n",
    "\n",
    "    valid_images = ['jpeg', 'jpg', 'png']\n",
    "    ext = image_path.split('.')[1]\n",
    "    \n",
    "    if ext.lower() in valid_images:\n",
    "        image = Image.open(image_path)\n",
    "        resized_image = resize(np.array(image), (180, 180, 3))  # Resize to match the model's input shape\n",
    "        \n",
    "        image_features = resized_image.flatten()\n",
    "        \n",
    "        # Pad or truncate the features to match the expected number\n",
    "        if len(image_features) < (180 * 180 * 3):\n",
    "            image_features = np.pad(image_features, (0, (180 * 180 * 3) - len(image_features)), mode='constant')\n",
    "        elif len(image_features) > (180 * 180 * 3):\n",
    "            image_features = image_features[:180 * 180 * 3]\n",
    "        \n",
    "        prediction = loaded_model.predict(image_features.reshape(1, 180, 180, 3))  # Reshape to match the model's input\n",
    "        \n",
    "        # Get the class with the highest probability\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        \n",
    "        if predicted_class == 0:\n",
    "            return 'Not binding'\n",
    "        elif predicted_class == 1:\n",
    "            return 'Yad Vashem binding'\n",
    "        else:\n",
    "            return 'Another binding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25f3de8d-af85-46e8-971a-4d925cb666c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type_pic_doc_ed(image_path):\n",
    "# Load the saved KNN model\n",
    "    # loaded_model = joblib.load('logistic_model.pkl')\n",
    "    loaded_model = tf.keras.models.load_model(\"pic-doc-ed.h5\")\n",
    "    # Loop through the images in the folder\n",
    "    valid_images = ['jpeg', 'jpg', 'png']  # Define the valid image extensions\n",
    "    ext = image_path.split('.')[1]\n",
    "    if ext.lower() in valid_images:\n",
    "        image = Image.open(image_path)\n",
    "        # Resize the image to match the expected input size of the SVM model\n",
    "        resized_image = resize(np.array(image), (50, 50))  # Resize to (50, 50) or adjust as needed\n",
    "    \n",
    "        # Flatten and add zeros to match the expected number of features\n",
    "        image_features = resized_image.flatten()\n",
    "        if len(image_features) < 6750:\n",
    "            image_features = np.pad(image_features, (0, 6750 - len(image_features)), mode='constant')\n",
    "        elif len(image_features) > 6750:\n",
    "            image_features = image_features[:6750]\n",
    "    \n",
    "                # Apply the loaded SVM model to the image\n",
    "        prediction = loaded_model.predict(image_features.reshape(1, -1))\n",
    "    \n",
    "        if prediction == 0:\n",
    "            return 'Testimony page'\n",
    "        elif prediction == 1:\n",
    "            return 'document'\n",
    "        else:\n",
    "            return 'Image'\n",
    "    # return 'Another binding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a8427d2-7358-4703-84ee-1d95c0dc7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x=check_type_pic_doc_ed('./pic.jpg')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c85cd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_Languages(data_response):\n",
    "    x = data_response['responses'][0]['fullTextAnnotation']['pages'][0]\n",
    "    if 'property' in x: \n",
    "        data_languages = data_response['responses'][0]['fullTextAnnotation']['pages'][0][\"property\"][\"detectedLanguages\"]\n",
    "        # print(data_languages)\n",
    "        # data_languages = data_languages['data_Languages']\n",
    "        # lang_map = {\n",
    "        #     \"de\" : \"German\", \n",
    "        #     \"es\" :\"Spanish\",\n",
    "        #     \"fr\" :\"French\", \n",
    "        #     \"it\" :\"Italian\", \n",
    "        #     \"ja\" :\"Japanese\", \n",
    "        #     \"ko\" :\"Korean\", \n",
    "        #     \"ru\" : \"Russian\", \n",
    "        #     \"zh\" :\"Chinese (Mandarin)\", \n",
    "        #     \"hr\" :\"Croatian\", \n",
    "        #     \"nl\": \"Dutch\", \n",
    "        #     'pl': 'Polish',\n",
    "        #     'en': 'English',\n",
    "        #     'sk': 'Slovak',\n",
    "        #     'de': 'German',\n",
    "        #     'da': 'Danish',\n",
    "        #     'no': 'Norwegian',\n",
    "        #     'lt': 'Lithuanian',\n",
    "        #     'haw': 'Hawaiian',\n",
    "        #     'mt': 'Maltese',\n",
    "        #     'ceb': 'Cebuano',\n",
    "        #     'uz': 'Uzbek', \n",
    "        #     'ja': 'Japanese',\n",
    "        #     'ko': 'Korean',\n",
    "        #     'ar': 'Arabic',\n",
    "        #     'ru': 'Russian',\n",
    "        #     'fr': 'French',\n",
    "        #     'iw':'Hebrew', \n",
    "        #     'yi':'Yiddish', \n",
    "        #     'fr':'French', \n",
    "        #     'de':'German',\n",
    "        #      'pl':'Polish',\n",
    "        #     'hu':'Hungarian', \n",
    "        #      'lb':'Luxembourgish',  \n",
    "        #     'lv':'Latvian',\n",
    "        #      'om':'Oromo', \n",
    "        #      'fy':'Frisian'\n",
    "        #     ,'it':'Italian',\n",
    "        #     'la':'Latin', \n",
    "        #      'sv' :'Swedish', \n",
    "        #      'nl':'Dutch', \n",
    "        #      'hr':'Croatian' \n",
    "        \n",
    "        #     # Add more languages as needed following the same pattern\n",
    "        # }\n",
    "        lang_map={\n",
    "    'iw':'Hebrew', \n",
    "    'af': 'Afrikaans',\n",
    "    'am': 'Amharic',\n",
    "    'ar': 'Arabic',\n",
    "    'az': 'Azerbaijani',\n",
    "    'be': 'Belarusian',\n",
    "    'bg': 'Bulgarian',\n",
    "    'bn': 'Bengali',\n",
    "    'bs': 'Bosnian',\n",
    "    'ca': 'Catalan',\n",
    "    'ceb': 'Cebuano',\n",
    "    'co': 'Corsican',\n",
    "    'cs': 'Czech',\n",
    "    'cy': 'Welsh',\n",
    "    'da': 'Danish',\n",
    "    'el': 'Greek',\n",
    "    'en': 'English',\n",
    "    'eo': 'Esperanto',\n",
    "    'et': 'Estonian',\n",
    "    'eu': 'Basque',\n",
    "    'fa': 'Persian',\n",
    "    'fi': 'Finnish',\n",
    "    'fj': 'Fijian',\n",
    "    'fo': 'Faroese',\n",
    "    'fy': 'Frisian',\n",
    "    'gd': 'Scottish Gaelic',\n",
    "    'gl': 'Galician',\n",
    "    'gu': 'Gujarati',\n",
    "    'ha': 'Hausa',\n",
    "    'haw': 'Hawaiian',\n",
    "    'he': 'Hebrew',\n",
    "    'hi': 'Hindi',\n",
    "    'hmn': 'Hmong',\n",
    "    'hr': 'Croatian',\n",
    "    'ht': 'Haitian Creole',\n",
    "    'hu': 'Hungarian',\n",
    "    'hy': 'Armenian',\n",
    "    'id': 'Indonesian',\n",
    "    'ig': 'Igbo',\n",
    "    'is': 'Icelandic',\n",
    "    'ja': 'Japanese',\n",
    "    'jv': 'Javanese',\n",
    "    'ka': 'Georgian',\n",
    "    'kk': 'Kazakh',\n",
    "    'km': 'Khmer',\n",
    "    'kn': 'Kannada',\n",
    "    'ko': 'Korean',\n",
    "    'ku': 'Kurdish',\n",
    "    'ky': 'Kyrgyz',\n",
    "    'la': 'Latin',\n",
    "    'lb': 'Luxembourgish',\n",
    "    'lo': 'Lao',\n",
    "    'lt': 'Lithuanian',\n",
    "    'lv': 'Latvian',\n",
    "    'mg': 'Malagasy',\n",
    "    'mi': 'Maori',\n",
    "    'mk': 'Macedonian',\n",
    "    'ml': 'Malayalam',\n",
    "    'mn': 'Mongolian',\n",
    "    'mr': 'Marathi',\n",
    "    'ms': 'Malay',\n",
    "    'mt': 'Maltese',\n",
    "    'my': 'Burmese',\n",
    "    'ne': 'Nepali',\n",
    "    'nl': 'Dutch',\n",
    "    'no': 'Norwegian',\n",
    "    'ny': 'Chichewa',\n",
    "    'or': 'Odia',\n",
    "    'pa': 'Punjabi',\n",
    "    'pl': 'Polish',\n",
    "    'ps': 'Pashto',\n",
    "    'pt': 'Portuguese',\n",
    "    'ro': 'Romanian',\n",
    "    'ru': 'Russian',\n",
    "    'rw': 'Kinyarwanda',\n",
    "    'sd': 'Sindhi',\n",
    "    'si': 'Sinhala',\n",
    "    'sk': 'Slovak',\n",
    "    'sl': 'Slovenian',\n",
    "    'sm': 'Samoan',\n",
    "    'sn': 'Shona',\n",
    "    'so': 'Somali',\n",
    "    'sq': 'Albanian',\n",
    "    'sr': 'Serbian',\n",
    "    'st': 'Sesotho',\n",
    "    'su': 'Sundanese',\n",
    "    'sv': 'Swedish',\n",
    "    'sw': 'Swahili',\n",
    "    'ta': 'Tamil',\n",
    "    'te': 'Telugu',\n",
    "    'tg': 'Tajik',\n",
    "    'th': 'Thai',\n",
    "    'tk': 'Turkmen',\n",
    "    'tl': 'Filipino',\n",
    "    'tr': 'Turkish',\n",
    "    'tt': 'Tatar',\n",
    "    'ug': 'Uyghur',\n",
    "    'uk': 'Ukrainian',\n",
    "    'ur': 'Urdu',\n",
    "    'uz': 'Uzbek',\n",
    "    'vi': 'Vietnamese',\n",
    "    'xh': 'Xhosa',\n",
    "    'yi': 'Yiddish',\n",
    "    'yo': 'Yoruba',\n",
    "    'zh': 'Chinese',\n",
    "    'zu': 'Zulu'\n",
    "}\n",
    "       \n",
    "        for la in data_languages:\n",
    "            i=la['languageCode']\n",
    "            if i in lang_map:\n",
    "                la['languageCode']=lang_map[i]\n",
    "        sorted_languages = sorted(data_languages, key=lambda x: x['confidence'], reverse=True)\n",
    "        \n",
    "        len_sorted = len(sorted_languages)\n",
    "        if len_sorted > 4:\n",
    "            len_sorted = 4\n",
    "        top_languages=[(la['languageCode'], la['confidence']) for la in sorted_languages[:len_sorted]]\n",
    "        # Extract the top 4 languages (or less if there are fewer than 4)\n",
    "        # top_languages = [(lang_map.get(lang['languageCode'], lang['languageCode']), lang['confidence']) for lang in sorted_languages[:len_sorted]]\n",
    "        \n",
    "        # Language code to language name mapping\n",
    "        return top_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "871d4dfe-ee1d-45fd-9392-68fc77b59e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "def write_to_csv_title(file_name):\n",
    "    with open(file_name, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Image Path', 'top_title'])\n",
    "\n",
    "def write_to_csv(data, file_name):\n",
    "    with open(file_name, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([data[0], data[1]])\n",
    "\n",
    "def read_from_csv(file_name):\n",
    "    with open(file_name, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            print(row)\n",
    "            return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e667ad3-4767-4504-a7fe-c87eabf0c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_dates_from_ocr_response(text):\n",
    "    date_formats = [\n",
    "        r'\\b\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}\\b',  # Date format: 28.05.1944 or 28/05/44\n",
    "        r'\\b\\d{1,2}\\.\\s*(?:Januar|Februar|März|April|Mai|Juni|Juli|August|September|Oktober|November|Dezember)\\s*\\d{2,4}\\b',  # Date format: 28. Mai 1944\n",
    "        r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s+\\d{1,2},?\\s+\\d{2,4}\\b', \n",
    "        r'\\b\\d{1,2}[./-]\\s*(?:Jan\\.?|Feb\\.?|Mar\\.?|Apr\\.?|May|Jun\\.?|Jul\\.?|Aug\\.?|Sep\\.?|Oct\\.?|Nov\\.?|Dec\\.?)\\s*\\d{2,4}\\b',  # Date format: 20. Jan. 1991\n",
    "        # Add more date formats as needed\n",
    "    ]\n",
    "\n",
    "    dates_found = []\n",
    "\n",
    "    for date_format in date_formats:\n",
    "        date_matches = re.finditer(date_format, text, re.IGNORECASE)\n",
    "        for date_match in date_matches:\n",
    "            dates_found.append(date_match.group())\n",
    "    # print(dates_found)\n",
    "    return dates_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f31de51-91bf-41b0-bdf8-350e320d7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dates_new(entity_types):\n",
    "    date_finish=[]\n",
    "    for type_name, entities in entity_types.items():\n",
    "        for en in entities:\n",
    "            en_split=en.split(' ')\n",
    "            for e_split in en_split:\n",
    "                date_found=extract_dates_from_ocr_response(e_split)\n",
    "                if len(date_found)>0:\n",
    "                    flag=False\n",
    "                    date=date_found[0]\n",
    "                    for d in date_finish:\n",
    "                        if d==date:\n",
    "                            flag=True\n",
    "                    if flag==False:\n",
    "                        date_finish.append(date)\n",
    "    return(date_finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a8817f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_format_json(img_path, json):\n",
    "    format_json = {}\n",
    "\n",
    "    # Check for the existence of 'full_desc_with_verices' key in the JSON\n",
    "    json_res = create_json_get_by_y(json)\n",
    "    \n",
    "    # Check if create_json_get_by_y operation was successful\n",
    "    if json_res:\n",
    "        format_json['full_desc_with_verices'] = json_res.get('full_desc_with_verices', [])\n",
    "        format_json['words'] = json_res.get('words', [])\n",
    "\n",
    "        # Process 'full_desc_with_verices' text and entities\n",
    "        if format_json['full_desc_with_verices']:\n",
    "            text_ = format_json['full_desc_with_verices'][0].get('text', '')\n",
    "            text_ = ' '.join(text_.split(\"\\n\")) if text_ else ''\n",
    "            format_json['full_desc_with_verices'][0]['text'] = text_\n",
    "            \n",
    "            entity_types = get_finds_dates_places_persons(text_)\n",
    "            arr_entity_types = []\n",
    "            cardinal_maiby_date=[]\n",
    "            date_arr = []\n",
    "            for type_name, entities in entity_types.items():\n",
    "                arr_entity_types.extend(entities)\n",
    "                format_json[type_name] = entities\n",
    "                if type_name == 'DATE':\n",
    "                    date_arr.extend(entities)\n",
    "                if type_name=='CARDINAL':\n",
    "                    cardinal_maiby_date.append(entities)\n",
    "            dates=append_dates_new(entity_types)\n",
    "            if dates!=[]:\n",
    "                for d in dates:\n",
    "                    date_arr.append(d)   \n",
    "            if date_arr==[]:\n",
    "                flag=False\n",
    "                res_date = get_date_try(json, date_arr,cardinal_maiby_date, flag )\n",
    "            else:\n",
    "                flag=True\n",
    "                res_date = get_date_try(json, date_arr,cardinal_maiby_date, flag )\n",
    "            if len(res_date)>0:\n",
    "                format_json['Production date'] = res_date\n",
    "            else:\n",
    "                format_json['Production date']=''\n",
    "            format_json['data_Languages'] = get_data_Languages(json)\n",
    "            \n",
    "            # Get title information using specific functions\n",
    "            dict_block_by_loc_title = get_title_try(json, json_res, arr_entity_types)\n",
    "            if dict_block_by_loc_title:\n",
    "                title = dict_block_by_loc_title[0]\n",
    "                format_json['top_title'] = title\n",
    "    # print(img_path, entity_types)\n",
    "    return format_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f3f89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_file(json_filename):\n",
    "    try:\n",
    "        with open(json_filename, 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f99ce010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_folders(main_folder):\n",
    "    valid_images = ['.json']  # Define the valid image extensions\n",
    "    imgs_dict = {}\n",
    "    regular_imgs=[]\n",
    "    to_remove=[]\n",
    "    for folder in os.listdir(main_folder):\n",
    "        folder_path = os.path.join(main_folder, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for image_file in os.listdir(folder_path):\n",
    "            ext = os.path.splitext(image_file)[1]\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            if ext.lower() not in valid_images:\n",
    "                regular_imgs.append(image_path)\n",
    "            else:\n",
    "                json=read_json_file(image_path)\n",
    "                if json!=None:\n",
    "                    imgs_dict[image_path]=json\n",
    "                else:\n",
    "                    to_remove.append(image_path.split('.')[0])\n",
    "    regular_imgs_2=[]\n",
    "    for i in regular_imgs:\n",
    "        ii=i.split('.')[0]\n",
    "        if ii not in to_remove:\n",
    "            regular_imgs_2.append(i)\n",
    "    return imgs_dict, regular_imgs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "81dfbf42-038b-4aba-9aa5-0bb05f138942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_for_json(json_filename, json_object):\n",
    "    with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(json_object, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b0191d4-06be-4424-9e8b-66a9a2fcf206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ed(class_names, img_height, img_width, pic_path, loaded_model):\n",
    "    print(pic_path)\n",
    "    img = tf.keras.utils.load_img(pic_path, target_size=(img_height, img_width))\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    predictions = loaded_model.predict(img_array)\n",
    "    \n",
    "    return class_names[np.argmax(predictions[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99905778-9a71-4c21-8ea7-af236b89b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_type_ed(image):\n",
    "    loaded_model = tf.keras.models.load_model(\"ed.h5\")\n",
    "    class_names = ['ed', 'edp']\n",
    "    img_height = 180\n",
    "    img_width = 180\n",
    "    if image.endswith(('.jpg', '.JPG', '.png', '.gif')):\n",
    "        pred = predict_ed(class_names, img_height, img_width, image, loaded_model)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09c4a537-a2bc-41b9-9383-44e88aa2d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pic_doc_ed(class_names, img_height, img_width, pic_path, loaded_model):\n",
    "    print(pic_path)\n",
    "    img = tf.keras.utils.load_img(pic_path, target_size=(img_height, img_width))\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    predictions = loaded_model.predict(img_array)\n",
    "    \n",
    "    return class_names[np.argmax(predictions[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "976c4c9a-fd32-4720-8721-147ff8734416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_type_pic_doc_ed(image):\n",
    "    loaded_model = tf.keras.models.load_model(\"pic-doc-ed.h5\")\n",
    "    class_names = ['pictures', 'documents', 'ed']\n",
    "    img_height = 180\n",
    "    img_width = 180\n",
    "    if image.endswith(('.jpg', '.JPG', '.png', '.gif')):\n",
    "        pred = predict_pic_doc_ed(class_names, img_height, img_width, image, loaded_model)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3bdf4243-92aa-4684-9bb2-f0b6fe78cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_binding(class_names, img_height, img_width, pic_path, loaded_model):\n",
    "    img = tf.keras.utils.load_img(pic_path, target_size=(img_height, img_width))\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    predictions = loaded_model.predict(img_array)\n",
    "    return class_names[np.argmax(predictions[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0d6f0388-9d0a-47b5-a8f5-3b996884ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_type_binding(image):\n",
    "    loaded_model = tf.keras.models.load_model(\"binding.h5\")\n",
    "    class_names = ['not binding', 'yad vashem binding', 'another binding']\n",
    "    img_height = 180\n",
    "    img_width = 180\n",
    "    if image.endswith(('.jpg', '.JPG', '.png', '.gif')):\n",
    "        pred = predict_binding(class_names, img_height, img_width, image, loaded_model)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bf209f3f-3014-467a-9366-0b731add451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_type_page(image_path):\n",
    "    pred_pic_doc_ed=predict_type_pic_doc_ed(image_path)\n",
    "    print(pred_pic_doc_ed)\n",
    "    if pred_pic_doc_ed=='pictures':\n",
    "        return 'picture'\n",
    "    elif pred_pic_doc_ed=='documents':\n",
    "        pred_bind=predict_type_binding(image_path)\n",
    "        print(pred_bind)\n",
    "        if pred_bind=='not binding':\n",
    "            return 'document'\n",
    "        else:\n",
    "            pred_bind\n",
    "    else:\n",
    "        pred_ed=predict_type_ed(image_path)\n",
    "        print(pred_ed)\n",
    "        if pred_ed=='ed':\n",
    "            return 'Testimony page'\n",
    "        else :\n",
    "            return 'Testimony page including a photograph'\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ba938f22-6e9e-4713-a1b1-e0fffa95b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_sql(json_data):\n",
    "    image_name=json_data.get(\"image_path\", '')\n",
    "    full_desc_with_verices=json_data.get(\"full_desc_with_verices\", '')\n",
    "    lang_text_1=''\n",
    "    lang_text_1+=str(full_desc_with_verices[0])\n",
    "    lang_text_1+=','\n",
    "    lang_text_1+=str(full_desc_with_verices[1])\n",
    "    full_desc_with_verices=lang_text_1\n",
    "    words=json_data.get(\"words\", '')\n",
    "    flat_data = [item for sublist in words for item in sublist]\n",
    "    long_string = ', '.join(map(str, flat_data))\n",
    "    words=long_string\n",
    "    top_title=json_data.get(\"top_title\", '')\n",
    "    format_json_path=json_data.get(\"format_json_path\", '')\n",
    "    json_path=json_data.get(\"json_path\", '')\n",
    "    date=json_data.get(\"Production date\", '')\n",
    "    if date==[]:\n",
    "        date=''\n",
    "    type_page=json_data.get(\"type_page\", '')\n",
    "# Establish connection to the SQL Server database\n",
    "    connection_string = 'DRIVER={SQL Server};SERVER=DESKTOP-E0FAPSB\\SQLEXPRESS;DATABASE=Practicom-DB-format_json;Trusted_Connection=yes;'\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Insert data into the Master table\n",
    "        cursor.execute(\"INSERT INTO Master (IMG_NAME, FULL_DESC_WITH_VERTICES, WORDS, DATE, JSON_PATH, FORMAT_JSON_PATH, TYPE_PAGE) VALUES (?, ?, ?, ?, ?, ?, ?)\", \n",
    "                           (image_name, full_desc_with_verices, words, date, json_path, format_json_path, type_page))\n",
    "        connection.commit()\n",
    "    \n",
    "        # Get the generated DOC_ID\n",
    "        cursor.execute(\"SELECT @@IDENTITY AS LastIdentity\")\n",
    "        doc_id = cursor.fetchone().LastIdentity\n",
    "    \n",
    "        print(doc_id)\n",
    "        # Insert data into the Title table with the correct DOC_ID\n",
    "        for title_text in top_title:\n",
    "            cursor.execute(\"INSERT INTO Title (DOC_ID, TITLE_TEXT) VALUES (?, ?)\", (doc_id, title_text))\n",
    "        connection.commit()\n",
    "    \n",
    "        # Insert data into the Data_Languages table\n",
    "        data_languages = json_data.get(\"data_Languages\", [])\n",
    "        for language, confidence in data_languages:\n",
    "            cursor.execute(\"INSERT INTO Data_Languages (DOC_ID, LANGUAGE, CONFIDENCE) VALUES (?, ?, ?)\", (doc_id, language, confidence))\n",
    "        connection.commit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during the database operations\n",
    "        print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "890c0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "def second_steps_for_all_images(data):\n",
    "    new_json_dict={}\n",
    "    regular_imgs=[]\n",
    "    imgs_dict, regular_imgs=process_images_in_folders(data)\n",
    "    img=''\n",
    "    for img_path, json in imgs_dict.items():\n",
    "        format_json=create_format_json(img_path, json)\n",
    "        format_json['json_path']=img_path\n",
    "        img=img_path.split('.')[0]\n",
    "        new_json_dict[img]=format_json\n",
    "    for img_path in regular_imgs:\n",
    "        format_json['image_path']=img_path\n",
    "        img1=img_path.split('.')[0]\n",
    "        format_json=new_json_dict[img1]\n",
    "        format_json['format_json_path']=img1+'_format.json'\n",
    "        format_json['page type']=check_type_page(img_path)\n",
    "        format_json['pic-doc-ed']=predict_type_pic_doc_ed(img_path)\n",
    "        write_to_sql(format_json)\n",
    "        write_for_json(format_json['format_json_path'], format_json)\n",
    "        print(img_path, format_json['Production date'], '  :::  ', format_json['top_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e4140e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d15ff864",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'responses'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43msecond_steps_for_all_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[99], line 9\u001b[0m, in \u001b[0;36msecond_steps_for_all_images\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      7\u001b[0m img\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path, json \u001b[38;5;129;01min\u001b[39;00m imgs_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 9\u001b[0m     format_json\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_format_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     format_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mimg_path\n\u001b[0;32m     11\u001b[0m     img\u001b[38;5;241m=\u001b[39mimg_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[75], line 5\u001b[0m, in \u001b[0;36mcreate_format_json\u001b[1;34m(img_path, json)\u001b[0m\n\u001b[0;32m      2\u001b[0m format_json \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check for the existence of 'full_desc_with_verices' key in the JSON\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m json_res \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_json_get_by_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Check if create_json_get_by_y operation was successful\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_res:\n",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m, in \u001b[0;36mcreate_json_get_by_y\u001b[1;34m(json)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_json_get_by_y\u001b[39m(json):\n\u001b[1;32m----> 2\u001b[0m     [words,full_desc_with_verices] \u001b[38;5;241m=\u001b[39m\u001b[43mget_all_words_with_vertices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     json_res\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m      4\u001b[0m     json_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_desc_with_verices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mfull_desc_with_verices\n",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m, in \u001b[0;36mget_all_words_with_vertices\u001b[1;34m(data_response)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_all_words_with_vertices\u001b[39m(data_response):\n\u001b[1;32m----> 2\u001b[0m     textAnnotations\u001b[38;5;241m=\u001b[39m\u001b[43mdata_response\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextAnnotations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m     text\u001b[38;5;241m=\u001b[39mdata_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfullTextAnnotation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m     arr_res\u001b[38;5;241m=\u001b[39m[]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'responses'"
     ]
    }
   ],
   "source": [
    "data='data'\n",
    "second_steps_for_all_images(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275f2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6d72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6734a7-f677-4c97-b744-7390a9d7bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data, file_name):\n",
    "    with open(file_name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Image Path', 'top_title'])\n",
    "        writer.writerow([data[0], data[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36268fdf-493d-4a37-ad4f-46bc06a8bcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fdd54-5ebc-47dd-9aaa-d466af556e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aca9c6-fda2-4a29-aadd-eb72cdb7449c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e7ad0-fdcc-4b28-93fe-3a5db6212cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5dcb33-e69d-421e-9075-40ff9b2101b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564c6af-6061-4797-bc20-302c09ce88a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239b0de-1e51-46ac-91ae-c2b2d13da2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
