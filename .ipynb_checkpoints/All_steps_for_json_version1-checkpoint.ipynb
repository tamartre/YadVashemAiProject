{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae90fbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\The user\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import os, os.path\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "import glob\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "00b195f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # לשנות:\n",
    "# את הלתך הראשון: בנפרד בלולאה\n",
    "# ואת טעינת הJSON - שלב 4. לעשות כן בלולאה. \n",
    "# לפצל את הפונקציה לתאריכים בנפרד ולכותרת\n",
    "# #     לכל תמונה שיעשה את כל השלבים:\n",
    "# # לעשות את 2 האפשרווית. : גם לרוץ על כל הJSON וגם על כל אחד בנפרד. \n",
    "# לשפר את הכותרת לפי כתב יד ועד דברים"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8928925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url=\"https://vision.googleapis.com/v1/images:annotate\"\n",
    "\n",
    "querystring={\"key\":\"AIzaSyBmcYM1GuYXf-ZPs5ppdFgLFSwQ5-LB33I\"}\n",
    "\n",
    "heders={\n",
    "    'Content-Type':\"application/json\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1480e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\the user\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\the user\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6a53bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\the user\\anaconda3\\lib\\site-packages (9.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b38be74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\the user\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2aebc",
   "metadata": {},
   "source": [
    "# step 1 cut platels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54440af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def cut_platels(img_path):\n",
    "    new_width = 400  # Specify the new width for resizing\n",
    "    new_height = 300  # Specify the new height for resizing\n",
    "    \n",
    "    template = cv2.imread('./colors.png')\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(\"Error: Could not read the image.\")\n",
    "        return\n",
    "\n",
    "    result = cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED)\n",
    "    (minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(result)\n",
    "    points = np.unravel_index(result.argmax(), result.shape)\n",
    "\n",
    "    if maxVal > 0.4:\n",
    "\n",
    "        # Calculate width of the template and remaining image\n",
    "        template_width = template.shape[1]\n",
    "        if points[0] > image.shape[0] / 2:\n",
    "            remaining_image_width = points[0] - 300\n",
    "        else:\n",
    "            remaining_image_width = image.shape[0] - (points[0] + 350)\n",
    "\n",
    "        print(\"Width of the template:\", template_width)\n",
    "        print(\"Width of the remaining image:\", remaining_image_width)\n",
    "\n",
    "        if points[0] > image.shape[0] / 2:\n",
    "            crop_img = image[0:points[0] - 300, :]\n",
    "        else:\n",
    "            crop_img = image[points[0] + 350:, :]\n",
    "\n",
    "        # Resize the cut image and the template\n",
    "        resized_cut_img = cv2.resize(crop_img, (new_width, new_height))\n",
    "        a(resized_cut_img)\n",
    "#         resized_template = cv2.resize(template, (new_width, new_height))\n",
    "\n",
    "        # Save the resized cut image\n",
    "        cv2.imwrite('resized_cut_image.jpg', resized_cut_img)\n",
    "        cv2.imshow('Resized Cut Image', resized_cut_img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Save the resized template\n",
    "        cv2.imwrite('resized_template.jpg', template)\n",
    "        cv2.imshow('Resized Template Image', template)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        crop_img = image\n",
    "\n",
    "    cropped_images_dict = {}\n",
    "    cropped_images_dict[img_path] = crop_img\n",
    "    return cropped_images_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59367eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(resized_cut_img) :   # Convert the resized cut image to grayscale\n",
    "    gray_resized_cut_img = cv2.cvtColor(resized_cut_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection on the resized cut image\n",
    "    edges_resized_cut_img = cv2.Canny(gray_resized_cut_img, 50, 150)\n",
    "\n",
    "    # Find contours to identify the object boundaries\n",
    "    contours, _ = cv2.findContours(edges_resized_cut_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Get the bounding box of the largest contour\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "\n",
    "    # Crop the resized cut image using the bounding box\n",
    "    cropped_resized_cut_img = resized_cut_img[y:y+h, x:x+w]\n",
    "\n",
    "    # Calculate the width of the cropped resized cut image\n",
    "    cropped_resized_cut_img_width = cropped_resized_cut_img.shape[1]\n",
    "    print(\"Width of the cropped resized cut image:\", cropped_resized_cut_img_width)\n",
    "\n",
    "    # Display the cropped resized cut image\n",
    "    cv2.imshow('Cropped Resized Cut Image', cropped_resized_cut_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "124670ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropped_images = cut_platels(\"00003 (2).jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b368707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_platels(imgs):\n",
    "    template = cv2.imread('./colors.png')\n",
    "    cropped_images_dict = {}\n",
    "    for img_path, img in imgs.items():\n",
    "        image = np.asarray(img)\n",
    "        result = cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED)\n",
    "        (minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(result)\n",
    "        points = np.unravel_index(result.argmax(), result.shape)\n",
    "\n",
    "        if maxVal > 0.4:\n",
    "            half = image.shape[0] / 2\n",
    "            if points[0] > half:\n",
    "                crop_img = image[0:points[0] - 300, :]\n",
    "                cropped_images_dict[img_path] = crop_img\n",
    "            else:\n",
    "                crop_img = image[points[0] + 350:, :]\n",
    "                cropped_images_dict[img_path] = crop_img\n",
    "        else:\n",
    "            cropped_images_dict[img_path] = image\n",
    "    return cropped_images_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed91dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_folders(main_folder):\n",
    "    valid_images = ['.jpeg', '.jpg', '.png']  # Define the valid image extensions\n",
    "    imgs_dict = {}\n",
    "\n",
    "    for folder in os.listdir(main_folder):\n",
    "        folder_path = os.path.join(main_folder, folder)\n",
    "        \n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for image_file in os.listdir(folder_path):\n",
    "            ext = os.path.splitext(image_file)[1]\n",
    "            if ext.lower() in valid_images:\n",
    "                image_path = os.path.join(folder_path, image_file)\n",
    "                img = Image.open(image_path)\n",
    "                imgs_dict[image_path] = img\n",
    "    return imgs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52f4a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_colors_1(data):\n",
    "    imgs = process_images_in_folders(data)\n",
    "    cropped_images=cut_platels(imgs)\n",
    "    return cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e81c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_data(image_data):\n",
    "    image = Image.fromarray(image_data)\n",
    "    b = io.BytesIO()\n",
    "    image.save(b, format='PNG')\n",
    "    im_bytes = b.getvalue()\n",
    "    \n",
    "    return base64.b64encode(im_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3278aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def img_request(encoded_image_data):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    \n",
    "    # Convert the encoded image data to base64 encoded string\n",
    "    encoded_image_str = encoded_image_data.decode('utf-8')\n",
    "    \n",
    "    payload = {\n",
    "        \"requests\": [{\n",
    "            \"image\": {\n",
    "                \"content\": encoded_image_str\n",
    "            },\n",
    "            \"features\": [{\n",
    "                \"type\": \"TEXT_DETECTION\"\n",
    "            }]\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url=url, json=payload, headers=headers, params=querystring)\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc00a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(json_filename):\n",
    "    with open(json_filename, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6d82b",
   "metadata": {},
   "source": [
    "# step 2 get ocr json and save in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6a49de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_cut_img_to_ocr_json_2(imgs):\n",
    "    for img_path, img_data in imgs.items():\n",
    "#         print(img_path)\n",
    "        dict_json_data={}\n",
    "        encoded_image = encode_image_data(img_data)\n",
    "        text = img_request(encoded_image)\n",
    "        dict_text = ast.literal_eval(text)\n",
    "        if \"responses\" not in dict_text or not dict_text[\"responses\"]:\n",
    "            continue\n",
    "        \n",
    "        detected_text = dict_text[\"responses\"][0][\"textAnnotations\"][0][\"description\"]\n",
    "        \n",
    "        # Save dict_text to a JSON file with the same name as the image path\n",
    "        json_filename = os.path.splitext(img_path)[0] + '.json'\n",
    "        with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(dict_text, json_file, ensure_ascii=False)\n",
    "        json_data = read_json_file(json_filename)\n",
    "        dict_json_data[json_filename]=json_data\n",
    "    return dict_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b23ea0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text_type(json):\n",
    "    handwritten_count = 0\n",
    "    computer_typed_count = 0\n",
    "    for text_annotation in json['responses'][0][\"textAnnotations\"]:\n",
    "        if \"detectedBreak\" in text_annotation:\n",
    "            text_detection_type = text_annotation[\"detectedBreak\"].get(\"type\", \"UNKNOWN\")\n",
    "            if text_detection_type == \"EOL_SURE_SPACE\":\n",
    "                handwritten_count += 1\n",
    "            else:\n",
    "                computer_typed_count += 1\n",
    "    \n",
    "    if handwritten_count > computer_typed_count:\n",
    "        return \"Handwritten\"\n",
    "    else:\n",
    "        return \"Computer-generated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23b813b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_Languages(data_response):\n",
    "    x=data_response['responses'][0]['fullTextAnnotation']['pages'][0][\"property\"][\"detectedLanguages\"]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "083d07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_with_vertices(data_response):\n",
    "    x=data_response['responses'][0][\"textAnnotations\"]\n",
    "    arr_res=[]\n",
    "    for idx, i in enumerate(x, start=0):\n",
    "        ver=i[\"boundingPoly\"]\n",
    "        desc=i['description']\n",
    "        desc=desc.split(\"\\n\")\n",
    "        json={\"ver\":ver, 'dsec':desc,  \"num\": idx}\n",
    "        arr_res.append(json);\n",
    "\n",
    "    return [arr_res[1:], arr_res[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea31c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words_in_json(json):\n",
    "    textAnnotations = json['textAnnotations']\n",
    "    word_count=0\n",
    "    for word in textAnnotations:\n",
    "        desc = word['description']\n",
    "        word_count+=1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f88000b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_document_title(data,word_count):\n",
    "    x=int(word_count/2)\n",
    "    # Assuming the document title is the text with the largest font size or at the top of the page\n",
    "    sorted_text_annotations = sorted(data, key=lambda x: x['ver']['vertices'][0]['y'] - x['ver']['vertices'][3]['y'])[:x]\n",
    "#     print(sorted_text_annotations)\n",
    "#     document_title = sorted_text_annotations[0]['dsec'][0] if sorted_text_annotations else \"No document title found\"\n",
    "    return sorted_text_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "052a7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_y(json):\n",
    "    y_start= json[\"full_desc_with_verices\"][\"ver\"][\"vertices\"][0][\"y\"]\n",
    "    y_end= json[\"full_desc_with_verices\"][\"ver\"][\"vertices\"][3][\"y\"]\n",
    "\n",
    "    range_= json[\"full_desc_with_verices\"][\"ver\"][\"vertices\"][3][\"y\"]-json[\"full_desc_with_verices\"][\"ver\"][\"vertices\"][0][\"y\"]\n",
    "\n",
    "    loc_title_up= (y_start,y_start+range_/4)\n",
    "    loc_title_middle=(y_start+range_/4,y_start+(range_/3)*2)\n",
    "    loc_title_down=(y_start+(range_/3)*2,y_start+range_)\n",
    "\n",
    "    return [loc_title_up,loc_title_middle,loc_title_down ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b3c8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_with_vertices_chani(data_response):\n",
    "    x=data_response[\"textAnnotations\"]\n",
    "    arr_res=[]\n",
    "    for idx, i in enumerate(x, start=0):\n",
    "        ver=i[\"boundingPoly\"]\n",
    "        desc=i['description']\n",
    "        desc=desc.split(\"\\n\")\n",
    "        json={\"ver\":ver, 'dsec':desc,  \"num\": idx}\n",
    "        arr_res.append(json);\n",
    "\n",
    "    return [arr_res[1:], arr_res[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e00badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_for_chani(json):\n",
    "    words=get_all_words_with_vertices_chani(json)\n",
    "    json2={}\n",
    "    json2[\"full_desc_with_verices\"]=words[1]\n",
    "    json2['words']=words[0]\n",
    "    return json2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d7c7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_block_areas_num_title(block_areas_num):\n",
    "    for i in block_areas_num:\n",
    "        print( 'desc:', i[0], 'letter', i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef3c4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sorted_max_font_size_title(sorted_max_font_size):\n",
    "    for i in sorted_max_font_size:\n",
    "        print( 'text:', i[1], 'letter_size:', i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05da890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_title(sorted_text_annotations, loc_title_up,loc_title_middle,loc_title_down ):\n",
    "    # for i in predicted_title:\n",
    "#     if i[\"num\"]<30:\n",
    "#         print('num: ', i[\"num\"] , '  dsec:', i[\"dsec\"] ,'  y= ', i['ver']['vertices'][0]['y']-i['ver']['vertices'][3]['y']) \n",
    "    block_areas = []\n",
    "    for i in sorted_text_annotations: \n",
    "#         block_areas.append((0,100))\n",
    "#           if i[\"num\"] < 30:\n",
    "        y = i['ver']['vertices'][3]['y'] - i['ver']['vertices'][0]['y']\n",
    "        desc=i[\"dsec\"]\n",
    "        if desc!='('  and desc!=')' :\n",
    "            if desc!='-' and desc!='[' and desc!=']' :\n",
    "                if desc!=',' and  desc!='.': \n",
    "                    try:\n",
    "                        int(desc[0])\n",
    "                        x=int(desc[0])\n",
    "                        if x not in range(0,10000):\n",
    "                            if i['ver']['vertices'][0]['y'] <= loc_title_up[1]:\n",
    "                                ans = 1\n",
    "                                block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "                            elif i['ver']['vertices'][3]['y'] <= loc_title_middle[1]:\n",
    "                                ans = 2\n",
    "                                block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "                            else:\n",
    "                                ans = 3\n",
    "                                block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "                    except ValueError:\n",
    "                        if i['ver']['vertices'][0]['y'] <= loc_title_up[1]:\n",
    "                            ans = 1\n",
    "                            block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "                        elif i['ver']['vertices'][3]['y'] <= loc_title_middle[1]:\n",
    "                            ans = 2\n",
    "                            block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "                        else:\n",
    "                            ans = 3\n",
    "                            block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "    return block_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9494f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predicted_title(predicted_title, loc_title_up,loc_title_middle,loc_title_down ): \n",
    "    for i in predicted_title:\n",
    "\n",
    "            y = i['ver']['vertices'][3]['y'] - i['ver']['vertices'][0]['y']\n",
    "            if i['ver']['vertices'][0]['y'] <= loc_title_up[1]:\n",
    "                ans = 1\n",
    "                print(\"loc_title: \", ans, 'num:', i[\"num\"], 'dsec:', i[\"dsec\"], 'y=', y)\n",
    "            elif i['ver']['vertices'][3]['y'] <= loc_title_middle[1]:\n",
    "                ans = 2\n",
    "                print(\"loc_title: \", ans, 'num:', i[\"num\"], 'dsec:', i[\"dsec\"], 'y=', y)\n",
    "            else:\n",
    "                ans = 3\n",
    "                print(\"loc_title: \", ans, 'num:', i[\"num\"], 'dsec:', i[\"dsec\"], 'y=', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "621b9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_list_in_block(blocks,block_areas, predicted_title ):\n",
    "#     רצים על כל מערך הכותרות. \n",
    "#     ובודקים אם המילים נמצאות. לכל מילה במערך מה ה-וואי שלה ומה גודל האותיות שלה. \n",
    "#     מחזירים מערך חדש. \n",
    "#  blocks====       max_font_size.append((words_vertices, text, letter_size))\n",
    "#   list1======    block_areas.append((desc,letter, block_vertices))\n",
    "# list 2======predicted_title.append({\"loc_title: \", ans, 'num:', i[\"num\"], 'dsec:', i[\"dsec\"], 'y=', y})\n",
    "    title_arr=[]\n",
    "    for block in blocks:\n",
    "        words=block[1]\n",
    "        words1=words.split(' ')\n",
    "        len_w=len(words1)\n",
    "        most=int(0.5*len_w)\n",
    "        try_words=0\n",
    "        sum_y=0\n",
    "        sum_letter=0\n",
    "        ans=1000\n",
    "        for word in words1:\n",
    "            for ba in block_areas:\n",
    "                if ba[0]==word:\n",
    "#                     print(word)\n",
    "                    try_words+=1\n",
    "                    sum_letter+=ba[1]\n",
    "            for pt in predicted_title:\n",
    "                if pt[2][0]==word:\n",
    "#                     print(word)\n",
    "                    sum_y+=pt[3]\n",
    "                    try_words+=1\n",
    "                    ans=pt[0]\n",
    "        if try_words>most:\n",
    "            if ans<1000:\n",
    "                title_arr.append((words1,ans,{'sum_y':sum_y, 'sum_letter':sum_letter}))\n",
    "    sorted_title_arr=sorted(title_arr, key=lambda x:sum_y , reverse=True)\n",
    "    return sorted_title_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47f19102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_title_arr(title_arr):\n",
    "    for i in title_arr:\n",
    "        print(\"*********\")\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b708990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all dates!\n",
    "import re\n",
    "import datetime as dt\n",
    "import re\n",
    "\n",
    "def extract_dates_from_ocr_response(text):\n",
    "    date_format = r'(\\b\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}\\b)|(\\b\\d{1,2}\\.\\s*(?:Januar|Juni|Februar|February|Februari|march|März|Marz|april|may|june|July august|september|october|november|december)\\s*\\d{2,4}\\b)|(\\b(?:january|februari|Juni|Februar|February|min|March|March|Marz|april|May|June|July|März|August|september|October|November|December)\\s+\\d{1,2},?\\s+\\d{2,4}\\b)|(\\b\\d{1,2}\\s(?:ינואר|פברואר|מרץ|אפריל|מאי|יוני|יולי|אוגוסט|ספטמבר|אוקטובר|נובמבר|דצמבר)\\s+\\d{2,4}\\b)|(\\d{1,2}\\s(?:ינואר|פברואר|מרץ|אפריל|מאי|יוני|יולי|אוגוסט|ספטמבר|אוקטובר|נובמבר|דצמבר)\\s+\\d{2,4}\\b)'\n",
    "\n",
    "    dates_found = []\n",
    "    date_matches = re.finditer(date_format, text, re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "    for date_match in date_matches:\n",
    "        dates_found.append(date_match.group())\n",
    "\n",
    "    return dates_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f75b0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_date_ans(words_vertices, loc_title_up,loc_title_middle,loc_title_down ):\n",
    "#     print(words_vertices)\n",
    "#     ans=0\n",
    "    \n",
    "# #     y = words_vertices['vertices'][3]['y'] - words_vertices['vertices'][0]['y']\n",
    "# #     if words_vertices['vertices'][0]['y'] <= loc_title_up[1]:\n",
    "# #         ans = 1\n",
    "# #     elif words_vertices['ver']['vertices'][3]['y'] <= loc_title_middle[1]:\n",
    "# #         ans = 2\n",
    "# #     else:\n",
    "# #         ans = 3\n",
    "#     return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "145e8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_date_in_block(sorted_max_font_size):\n",
    "#     date_arr=[]\n",
    "#     #  blocks====       max_font_size.append((words_vertices, text, letter_size, ans))\n",
    "#     for i in sorted_max_font_size:\n",
    "#         if i[3]!='non date':\n",
    "#             date_arr.append((i[1],ans))\n",
    "#     return date_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73159316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_title(sorted_text_annotations, loc_title_up,loc_title_middle,loc_title_down ):\n",
    "    # for i in predicted_title:\n",
    "#     if i[\"num\"]<30:\n",
    "#         print('num: ', i[\"num\"] , '  dsec:', i[\"dsec\"] ,'  y= ', i['ver']['vertices'][0]['y']-i['ver']['vertices'][3]['y']) \n",
    "    block_areas = []\n",
    "    for i in sorted_text_annotations: \n",
    "#         block_areas.append((0,100))\n",
    "#           if i[\"num\"] < 30:\n",
    "        y = i['ver']['vertices'][3]['y'] - i['ver']['vertices'][0]['y']\n",
    "        desc=i[\"dsec\"]\n",
    "        if desc!='('  and desc!=')' :\n",
    "            if desc!='-' and desc!='[' and desc!=']' :\n",
    "                if desc!=',' and  desc!='.': \n",
    "                    try:\n",
    "                        int(desc[0])\n",
    "                        x=int(desc[0])\n",
    "                        if x not in range(0,10000):\n",
    "                            if i['ver']['vertices'][0]['y'] <= loc_title_up[1]:\n",
    "                                ans = 1\n",
    "                                block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "                            elif i['ver']['vertices'][3]['y'] <= loc_title_middle[1]:\n",
    "                                ans = 2\n",
    "                                block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "                            else:\n",
    "                                ans = 3\n",
    "                                block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "                    except ValueError:\n",
    "                        if i['ver']['vertices'][0]['y'] <= loc_title_up[1]:\n",
    "                            ans = 1\n",
    "                            block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "                        elif i['ver']['vertices'][3]['y'] <= loc_title_middle[1]:\n",
    "                            ans = 2\n",
    "                            block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "                        else:\n",
    "                            ans = 3\n",
    "                            block_areas.append((ans,  i[\"num\"],  i[\"dsec\"],  y))\n",
    "    return block_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "03c519e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_ans(date_arr,words, loc_title_up,loc_title_middle,loc_title_downn):\n",
    "    new_date_arr=[]\n",
    "    for i in words:\n",
    "        desc=str(i[\"dsec\"][0])\n",
    "#         print('desc', desc,type(desc) )\n",
    "        for date in date_arr:\n",
    "#             print('date', date, type(date))\n",
    "            if desc==date:\n",
    "#                 print(\"OOOOOOOOOOOOO\")\n",
    "#               print(desc, date)\n",
    "                if i['ver']['vertices'][0]['y'] <= loc_title_up[1]:\n",
    "                    ans = 1\n",
    "                elif i['ver']['vertices'][3]['y'] <= loc_title_middle[1]:\n",
    "                    ans = 2\n",
    "                else:\n",
    "                    ans = 3 \n",
    "                new_date_arr.append((ans, date))\n",
    "    return new_date_arr\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4edf72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_steps(json):\n",
    "    dict_text=json['fullTextAnnotation']['text']\n",
    "    date_arr=extract_dates_from_ocr_response(dict_text)\n",
    "    print('date_arr', date_arr)\n",
    "    word_count=get_num_words_in_json(json) \n",
    "    json2=create_json_for_chani(json)\n",
    "    [loc_title_up,loc_title_middle,loc_title_down ]=get_size_y(json2)\n",
    "    json_response=json2[\"words\"]\n",
    "#     לפי גודל Y\n",
    "    sorted_text_annotations = predict_document_title(json_response,word_count )\n",
    "    predicted_title_2=get_predicted_title(sorted_text_annotations,loc_title_up,loc_title_middle,loc_title_down)\n",
    "#     לפי גודל אות\n",
    "    block_from_desc=get_big_letter_from_desc(json,word_count)\n",
    "#     לפי בלוק- מנסה לחשב גודל אות בבלוק. \n",
    "    sorted_max_font_size=get_big_letter_from_area(json,word_count)\n",
    "    title_arr=check_list_in_block(sorted_max_font_size,block_from_desc, predicted_title_2 )\n",
    "    new_date_arr=get_date_ans(date_arr, sorted_text_annotations,loc_title_up,loc_title_middle,loc_title_down)\n",
    "    print('new_date_arr', new_date_arr)\n",
    "#     date_arr=check_date_in_block(sorted_max_font_size)\n",
    "    print_title_arr(title_arr)\n",
    "#     print(date_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3c7e047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_3(new_json_dict):\n",
    "    for img_path, img_json in new_json_dict.items():\n",
    "        get_title_steps(img_json['responses'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d622ee",
   "metadata": {},
   "source": [
    "# step 4 get our json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fac997dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_ocr_json_to_formater_json_4(main_folder):\n",
    "    valid_images = ['.json']\n",
    "    # Define the valid image extensions\n",
    "    arr_json=[]\n",
    "    json_dict = {}\n",
    "    new_json_dict = {}\n",
    "    new_json={}\n",
    "    for folder in os.listdir(main_folder):\n",
    "        folder_path = os.path.join(main_folder, folder)\n",
    "        \n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for image_file in os.listdir(folder_path):\n",
    "            ext = os.path.splitext(image_file)[1]\n",
    "            if ext.lower() in valid_images:\n",
    "                json_filename = os.path.join(folder_path, image_file)\n",
    "                json_data = read_json_file(json_filename)\n",
    "                words=get_all_words_with_vertices(json_data)\n",
    "                new_json['json_path']=json_filename\n",
    "                new_json['data_Languages']=get_data_Languages(json_data)\n",
    "                new_json['full_desc_with_verices']=words[1]\n",
    "                new_json['words']=words[0]\n",
    "                new_json['text_type']=check_text_type(json_data)\n",
    "                x=json_filename.split('.')[0]\n",
    "                new_json['binding_type']=check_type_binding(x,main_folder)\n",
    "                \n",
    "                new_json_dict[json_filename]=new_json\n",
    "    arr_json.append(new_json_dict)\n",
    "    return arr_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "426c9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_type_binding(image_path_2, main_folder):\n",
    "# Load the saved KNN model\n",
    "#     loaded_model = joblib.load('knn_model.pkl')\n",
    "    # Loop through the images in the folder\n",
    "    valid_images = ['.jpeg', '.jpg', '.png']  # Define the valid image extensions\n",
    "    imgs_dict = {}\n",
    "\n",
    "    for folder in os.listdir(main_folder):\n",
    "        folder_path = os.path.join(main_folder, folder)\n",
    "        \n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for image_file in os.listdir(folder_path):\n",
    "            ext = os.path.splitext(image_file)[1]\n",
    "            if ext.lower() in valid_images:\n",
    "                image_path = os.path.join(folder_path, image_file)\n",
    "                image_path_1=image_path.split('.')[0]\n",
    "                if image_path_1==image_path_2:\n",
    "                    image = Image.open(image_path)\n",
    "                    # Resize the image to match the expected input size of the SVM model\n",
    "                    resized_image = resize(np.array(image), (50, 50))  # Resize to (50, 50) or adjust as needed\n",
    "\n",
    "                    # Flatten and add zeros to match the expected number of features\n",
    "                    image_features = resized_image.flatten()\n",
    "                    if len(image_features) < 6750:\n",
    "                        image_features = np.pad(image_features, (0, 6750 - len(image_features)), mode='constant')\n",
    "                    elif len(image_features) > 6750:\n",
    "                        image_features = image_features[:6750]\n",
    "\n",
    "                    # Apply the loaded SVM model to the image\n",
    "#                     prediction = loaded_model.predict(image_features.reshape(1, -1))\n",
    "\n",
    "#                     if prediction == 0:\n",
    "#                         print(f\"Image path {filename}: Type: Not binding\")       \n",
    "#                     elif prediction == 1:\n",
    "#                         print(f\"Image path {filename}: Type: Yad Vashem binding\")\n",
    "#                     else:\n",
    "#                         print(f\"Image path {filename}: Type: Another binding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "37faab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_to_csv(data_dict, file_name):\n",
    "    with open(file_name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Image Path', 'Words_desc','Words_area'])\n",
    "        for img_path, words_desc, words in data_dict.items():\n",
    "            writer.writerow([img_path, words[0],words[1]])\n",
    "\n",
    "def read_from_csv(file_name):\n",
    "    with open(file_name, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            print(row)\n",
    "\n",
    "# csv_file_name = 'image_data3.csv'\n",
    "\n",
    "# read_from_csv('image_data2.csv')\n",
    "# write_to_csv(dict_json_data, 'image_data2.csv')\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0871c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_blocks_with_max_area(json_data, words_count):\n",
    "    x=int(words_count/2)\n",
    "    block_areas = []\n",
    "    block_areas_num_only = []\n",
    "\n",
    "    fullTextAnnotation_pages = json_data['fullTextAnnotation']['pages']\n",
    "\n",
    "    for page in fullTextAnnotation_pages:\n",
    "        blocks = page['blocks']\n",
    "        for block in blocks:\n",
    "            block_vertices = block['boundingBox']['vertices']\n",
    "\n",
    "            x_values = [vertex['x'] for vertex in block_vertices]\n",
    "            max_x = max(x_values)\n",
    "            min_x = min(x_values)\n",
    "            y_values = [vertex['y'] for vertex in block_vertices]\n",
    "            max_y = max(y_values)\n",
    "            min_y = min(y_values)\n",
    "\n",
    "            block_x = max_x - min_x\n",
    "            block_y = max_y - min_y\n",
    "            area = block_x * block_y\n",
    "            block_areas.append((block, area))  # Store the block and its area\n",
    "            block_areas_num_only.append((area))  # Store the area only\n",
    "\n",
    "    # Sort blocks by area in descending order\n",
    "    sorted_blocks = sorted(block_areas, key=lambda x: x[1], reverse=True)[:x]\n",
    "    sorted_blocks_num_only = sorted(block_areas_num_only, reverse=True) # Get only the 5 highest area blocks\n",
    "\n",
    "    return sorted_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3dc5a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_big_letter_from_desc(json, words_count):\n",
    "    textAnnotations = json['textAnnotations']\n",
    "    block_areas = []\n",
    "    block_areas_num=[]\n",
    "    x=int(words_count/2)\n",
    "    for word in textAnnotations:\n",
    "        desc = word['description']\n",
    "        block_vertices = word['boundingPoly']['vertices']\n",
    "        \n",
    "        x_values = [vertex['x'] for vertex in block_vertices]\n",
    "        max_x = max(x_values)\n",
    "        min_x = min(x_values)\n",
    "        \n",
    "        y_values = [vertex['y'] for vertex in block_vertices]\n",
    "        max_y = max(y_values)\n",
    "        min_y = min(y_values)\n",
    "\n",
    "        block_x = max_x - min_x\n",
    "        block_y = max_y - min_y\n",
    "\n",
    "        num_letter = len(desc)\n",
    "        area = block_x * block_y\n",
    "        letter = area / num_letter\n",
    "        if desc!='('  and desc!=')' :\n",
    "            if desc!='-' and desc!='[' and desc!=']' :\n",
    "                if desc!=',' and  desc!='.': \n",
    "                    block_areas.append((desc, block_vertices))\n",
    "                    block_areas_num.append((desc, letter))\n",
    "    sorted_blocks = sorted(block_areas, key=lambda x: sum(vertex['y'] for vertex in x[1]) / 4, reverse=True)[:x]\n",
    "    sorted_block_areas_num = sorted(block_areas_num, key=lambda x:x[1], reverse=True)\n",
    "    return block_areas_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "92a6c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_block(block):\n",
    "    text=''\n",
    "    c=0\n",
    "    c_word=0\n",
    "    words=block['paragraphs'][0]['words']\n",
    "    c_word=len(words)\n",
    "    for w in words:\n",
    "        word=w['symbols']\n",
    "        for l in word:\n",
    "            text+=l['text']\n",
    "        text+=' '\n",
    "    text1=text.split(' '), \n",
    "    c+=len(text1)\n",
    "    for i in text1:\n",
    "        c+=len (i)\n",
    "    return [text, c, c_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6f266681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_text_from_block_1(block):\n",
    "#     text=''\n",
    "#     words=block['paragraphs'][0]['words']\n",
    "#     for w in words:\n",
    "#         word=w['symbols']\n",
    "#         for l in word:\n",
    "#             text+=l['text']\n",
    "#         text+=' '\n",
    "#     text1=text.split(' '), \n",
    "#     return text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "97fcdfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_handwritten(word_data):\n",
    "    if 'property' in word_data:\n",
    "        if 'detectedBreak' in word_data['property'] and 'detectedLanguages' in word_data['property']:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a4b38eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_with_max_font_size(sorted_blocks,words_count):\n",
    "    max_font_size = []\n",
    "    max_font_size_num = []\n",
    "    text=''\n",
    "    answer='non date'\n",
    "    for block, area in sorted_blocks:\n",
    "        words=block['paragraphs'][0]['words']\n",
    "        words_vertices=block['paragraphs'][0]['words'][0]['boundingBox']['vertices']\n",
    "        [text, len_block,  c_word]=get_text_from_block(block)\n",
    "#         if date_is!=0:\n",
    "#             ans=get_date_ans(words_vertices, )\n",
    "#             if ans==1:\n",
    "#                 answer='top date'\n",
    "#             elif ans==2:\n",
    "#                  answer='middle date'\n",
    "#             else:\n",
    "#                 answer='bottun date'\n",
    "                \n",
    "        letter_size = area / len_block\n",
    "        if words_count<50:\n",
    "            normal_words_num_in_block=0.1*words_count\n",
    "        else:\n",
    "            normal_words_num_in_block=0.3*words_count\n",
    "\n",
    "        if c_word<normal_words_num_in_block:\n",
    "            max_font_size.append((words_vertices, text, letter_size, answer))\n",
    "            max_font_size_num.append((letter_size,text))\n",
    "    \n",
    "    # Sort font sizes by size in descending order\n",
    "    sorted_max_font_size = sorted(max_font_size, key=lambda x: x[2], reverse=True)\n",
    "    sorted_max_font_size_num = sorted(max_font_size_num, reverse=True)\n",
    "    return sorted_max_font_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "97c3c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_big_letter_from_area(json, words_count):\n",
    "    sorted_blocks=find_blocks_with_max_area(json,words_count )\n",
    "    sorted_max_font_size=get_block_with_max_font_size(sorted_blocks,words_count)\n",
    "    return sorted_max_font_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "cdb52cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_steps_for_json():\n",
    "    data='data2'\n",
    "# #     step 1\n",
    "    cropped_images=cut_colors_1(data)\n",
    "# #     step 2\n",
    "    dict_json_data=from_cut_img_to_ocr_json_2(cropped_images)\n",
    "#     step 3\n",
    "    get_title_3(dict_json_data)\n",
    "# #     step 4\n",
    "    new_json_dict=from_ocr_json_to_formater_json_4(data)\n",
    "# #     step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fa85b",
   "metadata": {},
   "source": [
    "# all steps for json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2fd491f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_arr []\n",
      "new_date_arr []\n",
      "*********\n",
      "(['הקהילה', 'הגלילית', 'בב', 'בבליסטוק', ''], 1, {'sum_y': 259, 'sum_letter': 7560.571428571428})\n",
      "*********\n",
      "(['תעודת', '-', 'אישור', ''], 2, {'sum_y': 243, 'sum_letter': 2928.0})\n",
      "*********\n",
      "(['(', '-', ')', 'Dr.', 'Sz', 'Datner', ''], 3, {'sum_y': 432, 'sum_letter': 5745.333333333333})\n",
      "*********\n",
      "(['יושב', 'ראש', ':', 'ד\"ר', 'ש', '.', 'דטנר', ''], 2, {'sum_y': 68, 'sum_letter': 5719.5})\n"
     ]
    }
   ],
   "source": [
    "all_steps_for_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211c3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73bba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# שיבדוק לפי אם זה כתב יד או מכונה- וידע אם זה כותרת. \n",
    "# שידע לי על תאריכים ויחזיר אותם נורמלי. \n",
    "# שידע מה גודל תמונה לפני שחתכו אותה"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
